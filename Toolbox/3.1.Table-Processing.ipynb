{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: 1. Table-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From customized XML/HTML to customized key-value pairs\n",
    "#### 1. Run Table-Processing section powered GPT (Fill in your own OpenAI key and base_url, prompts, examples dicectly in code; Fill in examples for GPT in CSV)\n",
    "#### 2. Check GPT output Tables.\n",
    "#### 3. Sort, Clean and Integrate GPT-processed Tables (Remove File Name Folder, Table-Clean and integrate table1, table2, table3 ... into a JSON);\n",
    "#### Note: Examples for E are saved in FewShot-XML.csv, and examples for RSC and SN are saved in FewShot-HTML.csv\n",
    "#### Each customized key-value pairs are divided by \"###\", such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###<Title of the Table><Corresponding Row Header><Corresponding Column Header>=<Entry Value>###'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"###<Title of the Table><Corresponding Row Header><Corresponding Column Header>=<Entry Value>###\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Table-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_json_dirs: ['10.1007-s00170-019-04167-2']\n",
      "input_dir: 10.1007-s00170-019-04167-2\n",
      "current file: .DS_Store,index:1\n",
      "File .DS_Store moved to N folder because it doesn't contain 'Table'.\n",
      "current file: table1.json,index:1\n",
      "Processing file: table1.json\n",
      "submited table1.json\n",
      "predicted {\"Table 1: \"The chemical composition of the considered AA6061 aluminum alloy.\"\\n###<Material><AA6061><Composition (weight %)><Al>=<Base>###, \\n###<Material><AA6061><Composition (weight %)><Si>=<0.73>###, \\n###<Material><AA6061><Composition (weight %)><Fe>=<0.33>###, \\n###<Material><AA6061><Composition (weight %)><Cu>=<0.26>###, \\n###<Material><AA6061><Composition (weight %)><Mn>=<0.04>###, \\n###<Material><AA6061><Composition (weight %)><Mg>=<0.88>###, \\n###<Material><AA6061><Composition (weight %)><Cr>=<0.17>###, \\n###<Material><AA6061><Composition (weight %)><Zn>=<0.05>###, \\n###<Material><AA6061><Composition (weight %)><Ti>=<0.02>###, \\n###<Material><AA6061><Composition (weight %)><Others>=<0.02>###}\n",
      "Data/SN-GPT-Table/10.1007-s00170-019-04167-2\n",
      "：Completed，Save it as: Data/SN-GPT-Table/10.1007-s00170-019-04167-2table1.json\n",
      "current file: table2.json,index:1\n",
      "Processing file: table2.json\n",
      "submited table2.json\n",
      "predicted {\"Table 2: \"The mechanical properties of the employed AA6061 aluminum alloy along the rolling direction [18].\"\\n###<Youngâ€™s modulus (GPa)><74.2>###, \\n###<Yield strength (MPa)><349.2>###, \\n###<Tensile strength (MPa)><381.1>###, \\n###<Fracture elongation (%)><6.95>###}\n",
      "：Completed，Save it as: Data/SN-GPT-Table/10.1007-s00170-019-04167-2table2.json\n",
      "current file: table3.json,index:1\n",
      "Processing file: table3.json\n",
      "submited table3.json\n",
      "predicted {\"Table 3: \"The grain sizes of the samples shown in Figs. 15, 16, 17, and 18.\"\\n###<Sample><Ave. grain size><As received(Fig. 15)><>=<20.88>###, \\n###<Sample><Ave. grain size><As received(Fig. 15)><+ 50° SPIF>=<17.75>###, \\n###<Sample><Ave. grain size><As received(Fig. 15)><+ 70° SPIF>=<16.20>###, \\n###<Sample><Ave. grain size><Annealed(Fig. 16)><>=<20.40>###, \\n###<Sample><Ave. grain size><Annealed(Fig. 16)><+ 70° SPIF>=<19.13>###, \\n###<Sample><Ave. grain size><Annealed(Fig. 16)><+ 70° SPIF + T6*>=<23.14>###, \\n###<Sample><Ave. grain size><Solution treated(cycle II in Fig. 17)><+T6>=<20.48>###, \\n###<Sample><Ave. grain size><Solution treated(cycle I in Fig. 18)><+T4*>=<21.69>###, \\n###<Sample><Ave. grain size><Solution treated(cycle I in Fig. 18)><75 min–530 °C>=<19.85>###, \\n###<Sample><Ave. grain size><Solution treated(cycle I in Fig. 18)><75 min–545 °C>=<18.19>###, \\n###<Sample><Ave. grain size><Solution treated(cycle I in Fig. 18)><75 min–560 °C>=<15.75>###, \\n###<Sample><Ave. grain size><Solution treated(cycle I in Fig. 18)><75 min–545 °C+70° SPIF + T6>=<19.48>###}\n",
      "：Completed，Save it as: Data/SN-GPT-Table/10.1007-s00170-019-04167-2table3.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "from platform import system\n",
    "import time\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# -- global var start --\n",
    "api_key = \"...\" ### Fill in your own OpenAI key\n",
    "base_url = \"...\" ### Fill in base_url \n",
    "input_csv = 'Prompt+Example/Example/FewShot-HTML.csv'  # Examples for GPT, CSV file must contain 'Input column for GPT' and 'Output column for GPT';\n",
    "input_data_path = 'Data/SN-Table'  # Directory containing input table data\n",
    "output_path = 'Data/SN-GPT-Table'  # Directory to save the GPT-processed tables\n",
    "n_folder_path = 'Data/Failed-Table'  # Folder to store files without 'Table' in the name or failed tables\n",
    "system_content = \"\"\" This task is to take the simplified table webpage as input and convert each cell in the table body to customised format and \n",
    "output it as the json document. Please directly give the answer without any analysis.\"\"\" ### System instruction of your task, here is an example.\n",
    "system_instructions = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_content\n",
    "}\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "# -- global var end --\n",
    "\n",
    "\n",
    "# -- functions start --\n",
    "# 1. create user content\n",
    "def create_user_content():\n",
    "    # Read the CSV file containing I/O pairs\n",
    "    example_content = '''The customised format is <Table Title><Row Header><Column Header>=<Entry Value>, \n",
    "    and if the table has multilayered header, extract corresponding header from outside to inside. \n",
    "    2 examples are given, just help you to understand the customised format. Webpage that are slightly different in symbols need you to process.\n",
    "    Note:  1.colspan and rowspan mean the cell occupies multiple columns or rows, \n",
    "    2.morerows=x or morecols=x mean occupies (x+1) columns or rows,  \n",
    "    3.<entry>, <entry:&nbsp>, or no element exists after “:” mean a blank space should be leaved in the cell. ''' ### Your detailed task, here is an example.\n",
    "    with open(input_csv, mode='r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        # Collecting I/O pairs from CSV\n",
    "        example_num = 1\n",
    "        for row in csv_reader:\n",
    "            ex1 = row[\"Input\"],\n",
    "            ex1_output = row[\"Output\"],\n",
    "            example_content += f\"\"\"### Example {example_num}:\\nInput:\\n{ex1}\\nOutput:\\n{ex1_output}\\n\"\"\"\n",
    "            example_num += 1\n",
    "    return example_content\n",
    "\n",
    "\n",
    "# 2. detect_file_encoding\n",
    "def detect_file_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "# 3 .get root dirs\n",
    "def get_dirs(root_dir):\n",
    "    dirs = os.listdir(root_dir)\n",
    "    dirs.remove('.DS_Store')\n",
    "    return dirs\n",
    "\n",
    "# 4. detect_file_exist\n",
    "def detect_file_exist(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# -- functions end --\n",
    "def few_shot_from_csv(input_csv, input_data_path, output_path, api_key, n_folder_path):\n",
    "    # Process new input data and get prediction\n",
    "    file_list = os.listdir(input_data_path)  # Load input data\n",
    "    flag = 1\n",
    "    for file in file_list:\n",
    "        print(f\"current file: {file},index:{flag}\")\n",
    "        if detect_file_exist(os.path.join(output_path,file)):\n",
    "            print(f\" {os.path.join(output_path,file)} Exist\")\n",
    "            continue\n",
    "        messages = [system_instructions]  # Add system instruction first\n",
    "        user_instructions = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": create_user_content()\n",
    "        }\n",
    "        messages.append(user_instructions)\n",
    "\n",
    "        # Maintain original file names\n",
    "        input_file_path = os.path.join(input_data_path, file)\n",
    "        output_file_name = os.path.splitext(file)[0] + '.json'\n",
    "        output_file_path = os.path.join(output_path, output_file_name)\n",
    "        encodeing = detect_file_encoding(input_file_path)\n",
    "        # Read file content\n",
    "        with open(input_file_path, 'r', encoding=encodeing) as input_file:\n",
    "            text = input_file.read()\n",
    "            # print(text)\n",
    "\n",
    "        # Check if file content contains 'Table'\n",
    "        if 'Table' not in text and 'table' not in text:\n",
    "            # Move file to 'N' folder without processing\n",
    "            if not os.path.exists(n_folder_path):\n",
    "                os.makedirs(n_folder_path)\n",
    "            os.rename(input_file_path, os.path.join(n_folder_path, file))\n",
    "            print(f\"File {file} moved to N folder because it doesn't contain 'Table'.\")\n",
    "            continue  # Skip to the next file\n",
    "\n",
    "        # If 'Table' is found in the content, proceed with processing\n",
    "        print(f\"Processing file: {file}\")\n",
    "\n",
    "        # Add the new input to be processed by the model\n",
    "        messages.append({\"role\": \"user\", \"content\": text})\n",
    "        print(f\"submited {file}\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            #    frequency_penalty=0,\n",
    "            #    presence_penalty=0\n",
    "            # max_completion_tokens = 8192\n",
    "        )\n",
    "\n",
    "        prediction = response.choices[0].message.content.strip()\n",
    "        print(f\"predicted {prediction}\")\n",
    "        send_num = 1\n",
    "        max_retries = 3\n",
    "        retries = 0\n",
    "        while True:\n",
    "\n",
    "            if \"...\" in prediction:\n",
    "                send_num += 1\n",
    "                print(f\"Omit,ask again : {send_num}\")\n",
    "                if send_num >= 3:\n",
    "                    messages = messages[0:-3]\n",
    "                messages.append({\"role\": \"assistant\", \"content\": prediction})\n",
    "                messages.append({\"role\": \"user\",\n",
    "                                 \"content\": \"Do not omit and Continue your output, each cell should be converted !!!\"})\n",
    "\n",
    "                if retries < max_retries:\n",
    "                    try:\n",
    "                        response = client.chat.completions.create(\n",
    "                            model=\"gpt-4-1106-preview\",\n",
    "                            messages=messages,\n",
    "                            temperature=0.3,\n",
    "                            #        frequency_penalty=0,\n",
    "                            #        presence_penalty=0,\n",
    "                            #        max_completion_tokens = 8192\n",
    "                        )\n",
    "                        prediction = response.choices[0].message.content.strip()\n",
    "\n",
    "                    except openai.InternalServerError as e:\n",
    "                        retries += 1\n",
    "                        print(f\"Omit：{e}。Retrying... ({retries}/{max_retries})\")\n",
    "                        time.sleep(2 ** retries)  # 指数回退延迟\n",
    "                        if retries == max_retries:\n",
    "                            raise Exception(\"Maximized Retry times. Overloaded\")\n",
    "            else:\n",
    "                # print(\"omit not in prediction,ask again\")\n",
    "                # print(\"：Complete，Save it\")\n",
    "                break\n",
    "        # print(prediction)\n",
    "        if not os.path.exists(output_path):\n",
    "            print(output_path)\n",
    "            os.mkdir(output_path)\n",
    "        with open(os.path.join(output_path,file), \"w\") as f:\n",
    "            f.write(prediction)\n",
    "        print(\"：Completed，Save it as:\", output_path + file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to set up parameters, read configuration, and invoke the few-shot learning process.\n",
    "    \"\"\"\n",
    "    # Ensure output and 'N' paths exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    if not os.path.exists(n_folder_path):\n",
    "        os.makedirs(n_folder_path)\n",
    "\n",
    "    input_json_dirs = get_dirs(input_data_path)\n",
    "    print(\"input_json_dirs:\",input_json_dirs)\n",
    "    for input_dir in input_json_dirs:\n",
    "        print(\"input_dir:\",input_dir)\n",
    "        input_json_path = os.path.join(input_data_path, input_dir)\n",
    "        output_json_path = os.path.join(output_path, input_dir)\n",
    "        few_shot_from_csv(input_csv, input_json_path, output_json_path, api_key, n_folder_path)\n",
    "\n",
    "    # Call the few-shot function to process the CSV and input data\n",
    "    #few_shot_from_csv(input_csv, input_data_path, output_path, api_key, n_folder_path)\n",
    "\n",
    "# Run the main function if this script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove File Name Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming Data/RSC-GPT-Table/10.1039-b811775f/table1.json to Data/RSC-GPT-Table/10.1039-b811775f-table1.json\n",
      "Successfully renamed: Data/RSC-GPT-Table/10.1039-b811775f/table1.json to Data/RSC-GPT-Table/10.1039-b811775f-table1.json\n",
      "Renaming Data/RSC-GPT-Table/10.1039-b811775f/table2.json to Data/RSC-GPT-Table/10.1039-b811775f-table2.json\n",
      "Successfully renamed: Data/RSC-GPT-Table/10.1039-b811775f/table2.json to Data/RSC-GPT-Table/10.1039-b811775f-table2.json\n",
      "Renaming Data/RSC-GPT-Table/10.1039-b811775f/table3.json to Data/RSC-GPT-Table/10.1039-b811775f-table3.json\n",
      "Successfully renamed: Data/RSC-GPT-Table/10.1039-b811775f/table3.json to Data/RSC-GPT-Table/10.1039-b811775f-table3.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the base directory (replace with your actual base path)\n",
    "base_dir = r'Data/RSC-GPT-Table'  # Put Your Own Table directory \n",
    "\n",
    "# Walk through all directories and files in the base directory\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        # Check if the file is a JSON file\n",
    "        if file.endswith(\".json\"):\n",
    "            # Get the DOI folder name\n",
    "            doi_folder_name = os.path.basename(root)\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Define new file name: DOI_foldername + original file name\n",
    "            new_file_name = f\"{doi_folder_name}-{file}\"\n",
    "            new_file_path = os.path.join(base_dir, new_file_name)\n",
    "            \n",
    "            print(f\"Renaming {file_path} to {new_file_path}\")\n",
    "            try:\n",
    "                # Rename the JSON file\n",
    "                os.rename(file_path, new_file_path)\n",
    "                print(f\"Successfully renamed: {file_path} to {new_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to rename {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
