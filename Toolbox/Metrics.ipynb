{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Results:\n",
      "Token-level Metrics:\n",
      "Token Precision: 0.8018\n",
      "Token Recall: 0.8593\n",
      "Token F1-Score: 0.8295\n",
      "Token Accuracy: 0.7087\n",
      "Token TP: 635, FP: 157, FN: 104\n",
      "\n",
      "Character-level Metrics:\n",
      "Char Precision: 0.8957\n",
      "Char Recall: 0.9276\n",
      "Char F1-Score: 0.9114\n",
      "Char Accuracy: 0.8372\n",
      "Char TP: 3832, FP: 446, FN: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/2gvzynk95l5bq44vwtsyyqrm0000gn/T/ipykernel_29377/1412820351.py:503: DtypeWarning: Columns (1,2,3,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  extracted_df = pd.read_csv(config[\"csv_file_path\"])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "###  *****   Config *****\n",
    "config = {\n",
    "    \"json_file_path\": \"/Users/zixuanzhao/Desktop/Metrics/GroundTruth/GroundTruth_Strengthening-Mechanism.json\",    ### input_json\n",
    "    \"csv_file_path\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Text/Spark-4.0Ultra.csv\",  ### input_csv\n",
    "    \"output_csv_path\": \"/Users/zixuanzhao/Desktop/Metrics/GroundTruth/GroundTruth_Strengthening-Mechanism.csv\",  ### output_csv\n",
    "    \"output_columns\": {\n",
    "        # \"groundtruth_column\": \"GroundTruth_Composition\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Composition\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Composition\" ### input_csv_extracted_column\n",
    "        \n",
    "        # \"groundtruth_column\": \"GroundTruth_Heat_treatment\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Heat_treatment\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Heat_Treatment\" ### input_csv_extracted_column\n",
    "        \n",
    "        # \"groundtruth_column\": \"GroundTruth_Heat_treatment2\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Heat_treatment2\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Deformation/Manufacturing_Technique\" ### input_csv_extracted_column\n",
    "        \n",
    "        # \"groundtruth_column\": \"GroundTruth_Microstructure1\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Microstructure1\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Microstructure1\" ### input_csv_extracted_column\n",
    "        \n",
    "        # \"groundtruth_column\": \"GroundTruth_Microstructure2\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Microstructure2\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Microstructure2\" ### input_csv_extracted_column\n",
    "        \n",
    "        # \"groundtruth_column\": \"GroundTruth_Properties\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Properties\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Properties\" ### input_csv_extracted_column\n",
    "        \n",
    "        # \"groundtruth_column\": \"GroundTruth_Performance\",  ### output_csv_groundtruth_column\n",
    "        # \"extracted_column\": \"Extracted_Performance\",   ### output_csv_extracted_column\n",
    "        # \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        # \"csv_column\": \"Performance\" ### input_csv_extracted_column\n",
    "        \n",
    "        \"groundtruth_column\": \"GroundTruth_Strengthening-Mechanism\",  ### output_csv_groundtruth_column\n",
    "        \"extracted_column\": \"Extracted_Strengthening-Mechanism\",   ### output_csv_extracted_column\n",
    "        \"id_column\": \"ID\", ### output_csv_id_column\n",
    "        \"csv_column\": \"Strengthening_Mechanism\" ### input_csv_extracted_column\n",
    "    },\n",
    "    \"line_starts_with\": {\n",
    "        \"id\": '{{\"ID\":',  ### input_json_ID_indictor\n",
    "        # \"groundtruth\": '{\"Structured_Composition\":', ### input_json_Structured_Composition_indictor\n",
    "        # \"groundtruth\": '{\"Heat_Treatment\":', ### input_json_Structured_Processing_indictor\n",
    "        # \"groundtruth\": '{\"Deformation/Manufacturing_Technique\":', ### input_json_Structured_Deformation_indictor\n",
    "        # \"groundtruth\": '{\"Structured_Microstructure1\":', ### input_json_Structured_Microstructure1_indictor\n",
    "        # \"groundtruth\": '{\"Structured_Microstructure2\":', ### input_json_Structured_Microstructure2_indictor\n",
    "        # \"groundtruth\": '{\"Structured_Properties\":', ### input_json_Structured_Properties_indictor\n",
    "        # \"groundtruth\": '{\"Structured_Performance\":', ### input_json_Structured_Performance_indictor\n",
    "        \"groundtruth\": '{\"Structured_Strengthening_Mechanism\":', ### input_json_Structured_Strengthening-Mechanism_indictor\n",
    "    },\n",
    "    \"id_pattern\": r'\"ID\":\\s*(\\d+)',  # Pattern to extract ID\n",
    "    \"json_block_pattern\": r'```(.*?)```', # New pattern to extract JSON blocks between ```{ and }```\n",
    "    # \"json_extraction_pattern\": r'\"Structured_Composition\":\\s*\"(.*?)\"}}', # Pattern to extract Structured_Composition\n",
    "    # \"json_extraction_pattern\": r'\\{\"Heat_Treatment\":(.*?)(?:,\"Deformation/Manufacturing_Technique\"|$)|^N/A$', # Pattern to extract Heat_Treatment\n",
    "    # \"json_extraction_pattern\": r'\\{\"Deformation/Manufacturing_Technique\":(.*?)(?:}}|$)|^N/A$', # Pattern to extract Structured_Deformation\n",
    "    # \"json_extraction_pattern\": r'\\{\"Structured_Microstructure1\":(.*?)(?:}}|$)|^N/A$', # Pattern to extract Structured_Microstructure1\n",
    "    # \"json_extraction_pattern\": r'\\{\"Structured_Microstructure2\":(.*?)(?:}}|$)|^N/A$', # Pattern to extract Structured_Microstructure2\n",
    "    # \"json_extraction_pattern\": r'\\{\"Structured_Properties\":(.*?)(?:,\"Structured_ Performance\"|$)|^N/A$', # Pattern to extract Properties\n",
    "    # \"json_extraction_pattern\": r'\\{\"Structured_ Performance\":(.*?)(?:}}|$)|^N/A$', # Pattern to extract Structured_Performance\n",
    "    \"json_extraction_pattern\": r'\\{\"Structured_Strengthening_Mechanism\":(.*?)(?:}}|$)|^N/A$', # Pattern to extract Structured_Strengthening-Mechanism\n",
    "        \n",
    "    \"preprocess_patterns\": [\n",
    "        (r'\"[^\"]+\":\"{}\"', ''),\n",
    "        (r'\"[^\"]+\":\"N/A\"', ''),\n",
    "        (r'\"[^\"]+\":\"{N/A}\"', ''),\n",
    "        (r'{value/unit}', 'N/A'),\n",
    "        (r'\"( Density|Melting point|Electrical Conductivity|Electrical resistivity|Magnetic Properties|Saturation Magnetization/Ms|Thermodynamic Properties|Mixing Enthalpy|Curie Temperature|Object|Transformation temperature|Mixing Enthalpy|Mixing Entropy|Heat Capacity|Thermal conductivity|Thermal expansion|Thermal diffusivity|Remanent Magnetic Induction/Br|Coercivity/Hc|Saturation Magnetization/M|Magnetization Finish/Mf|Description|YS|UTS|Elongation|Compressive Strength|Compressive strain|Fracture strength|Fracture strain|Reduction of area|Hardness|Young’s modulus|Shear modulus|Flexural modulus|Poisson’s ratio)\": \\{\\},?\\s*', ''), \n",
    "        (r'\"Mechanical Properties\": {N/A}', ''),\n",
    "        (r'\"Physical Properties\": {N/A}', ''),\n",
    "        (r'\"Magnetic Properties\": {N/A}', ''),\n",
    "        (r'\"Thermodynamic Properties\": {N/A}', ''),\n",
    "        (r'\"Mechanical Properties\":{N/A}', ''),\n",
    "        (r'\"Physical Properties\":{N/A}', ''),\n",
    "        (r'\"Magnetic Properties\":{N/A}', ''),\n",
    "        (r'\"Thermodynamic Properties\":{N/A}', ''),\n",
    "        (r'\"Mechanical Properties\":\"{N/A}\"', ''),\n",
    "        (r'\"Physical Properties\":\"{N/A}\"', ''),\n",
    "        (r'\"Magnetic Properties\":\"{N/A}\"', ''),\n",
    "        (r'\"Thermodynamic Properties\":\"{N/A}\"', ''),\n",
    "        (r'\"Properties\": \"N/A\"', 'N/A'),\n",
    "        (r'\"Mechanical Properties\":\"{}\"', ''),\n",
    "        (r'\"Physical Properties\":\"{}\"', 'N/A'),\n",
    "        (r'\"Magnetic Properties\":\"{}\"', 'N/A'),\n",
    "        (r'\"Thermodynamic Properties\":\"{}\"', 'N/A'),        \n",
    "        (r'\"Others\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Others\":\\s*\\{.*?\\},?',''),\n",
    "        (r'\"Others\":\"\\{.*?\\}\",?', ''),                                  \n",
    "        (r'\"Others\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Others\":\"N/A\"', ''),\n",
    "        (r'\"Others\": \"N/A\"', ''),\n",
    "        (r'\"Others\":\"[^\"]*\",?', ''),\n",
    "        (r'\"Other Properties\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Other Properties\":\\s*\\{.*?\\},?',''),\n",
    "        (r'\"Other Properties\":\"\\{.*?\\}\",?', ''),                                  \n",
    "        (r'\"Other Properties\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Other Properties\":\"N/A\"', ''),\n",
    "        (r'\"Other Properties\": \"N/A\"', ''),\n",
    "        (r'\"Other Properties\":\"[^\"]*\",?', ''),\n",
    "        (r'\"Property Name\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Property Name\":\\s*\\{.*?\\},?',''),\n",
    "        (r'\"Property Name\":\"\\{.*?\\}\",?', ''),                                  \n",
    "        (r'\"Property Name\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Property Name\":\"N/A\"', ''),\n",
    "        (r'\"Property Name\": \"N/A\"', ''),\n",
    "        (r'\"Property Name\":\"[^\"]*\",?', ''),\n",
    "        (r'\"Property Value\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Property Value\":\\s*\\{.*?\\},?',''),\n",
    "        (r'\"Property Value\":\"\\{.*?\\}\",?', ''),                                  \n",
    "        (r'\"Property Value\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Property Value\":\"N/A\"', ''),\n",
    "        (r'\"Property Value\": \"N/A\"', ''),\n",
    "        (r'\"Property Value\":\"[^\"]*\",?', ''),\n",
    "        (r'\"Other Parameters\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Description\":\"{N/A}\"', ''),  \n",
    "        (r'\"Characteristic\":\\s*\".*?\",?', ''),\n",
    "        (r'\"Solute-Atoms\": \"N/A\",', ''),\n",
    "        (r'\"Grain Characteristics\": \"N/A\",', ''),\n",
    "        (r'\"Dislocation\": \"N/A\",', ''),\n",
    "        (r'\"Defect\": \"N/A\",', ''),\n",
    "        (r'\"Texture\": \"N/A\",', ''),\n",
    "        (r'\"Microstructure\": \"N/A\",', ''),\n",
    "        (r'\"Zones and Deformation Bands\": \"N/A\",', ''),\n",
    "        (r'\"Fracture\": \"N/A\",', ''),\n",
    "        (r'\"Crack\": \"N/A\"', ''),\n",
    "        (r'\"Grain Boundary\": \"N/A\",', ''),\n",
    "        (r'\"Segregation\": \"N/A\",', ''),\n",
    "        (r'{}', 'N/A'),\n",
    "        (r'Here\\s+(is|are)\\s+.*\\s+(JSON\\s+format|output|text|treatment|representation)\\s*:', ''),\n",
    "        (r'Here\\s+(is|are)\\s+.*\\s+(JSON)\\s*:', ''),\n",
    "        (r'(Based on)\\s+the\\s+.*\\s+(provided\\s+text|information)\\s*:', ''),\n",
    "        (r'Based\\s+on\\s+.*\\s+(JSON\\s+format|output|text)\\s*:', ''),\n",
    "        (r'Here\\s+(is|are)\\s+the\\s+customized\\s+JSON\\s+for\\s+.*?:', ''),   \n",
    "        (r'Note:\\s+.*?\\..*?\\.', ''),\n",
    "        (r'Note:\\s+.*?\\.', ''),\n",
    "        (r'\"Melting\": {\"Method\": \"N/A\",\"Equipment\": \"N/A\",\"Temperature\": \"N/A\",\"Atmosphere\": \"N/A\"}', ''),    \n",
    "        (r'\"Melting\": {\"Method\": \"N/A\",\"Equipment\": \"N/A\",\"Temperature\": \"N/A\"}', ''),\n",
    "        (r'\"Casting\": {\"Method\": \"N/A\",\"Temperature\": \"N/A\",\"Diameter\": \"N/A\"}', ''),        \n",
    "        (r'\"Austenitization\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Transformation\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),        \n",
    "        (r'\"Homogenization\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"},', ''),\n",
    "        (r'\"Solution\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\",\"HeatingRate\": \"N/A\"}', ''),        \n",
    "        (r'\"Isothermal Aging\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Pre-Aging\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),        \n",
    "        (r'\"Retrogression\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Re-aging\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),            \n",
    "        (r'\"Natural Aging\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Creep Aging\": {\"Temperature\": \"N/A\",\"Time\": \"N/A\",\"Stress\": \"N/A\"}', ''), \n",
    "        (r'\"Melting\": {\"Method\": \"{N/A}\",\"Equipment\": \"{N/A}\",\"Temperature\": \"{N/A}\",\"Atmosphere\": \"{N/A}\"}', ''),\n",
    "        (r'\"Casting\": {\"Method\": \"{N/A}\",\"Temperature\": \"{N/A}\",\"Diameter\": \"{N/A}\"}', ''), \n",
    "        (r'\"Austenitization\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Transformation\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''), \n",
    "        (r'\"Homogenization\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"},', ''),\n",
    "        (r'\"Solution\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\",\"HeatingRate\": \"{N/A}\"}', ''), \n",
    "        (r'\"Isothermal Aging\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Pre-Aging\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''), \n",
    "        (r'\"Retrogression\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Re-aging\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''), \n",
    "        (r'\"Natural Aging\": {\"Temperature\": \"{N/A}\",\"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Creep Aging\": {\"Temperature\": \"{{N/A}}\",\"Time\": \"{{N/A}}\",\"Stress\": \"{N/A}\"}', ''),  \n",
    "        (r'\"Melting\": {\"Method\": \"N/A\", \"Equipment\": \"N/A\", \"Temperature\": \"N/A\", \"Atmosphere\": \"N/A\"}', ''),    \n",
    "        (r'\"Melting\": {\"Method\": \"N/A\", \"Equipment\": \"N/A\", \"Temperature\": \"N/A\"}', ''),\n",
    "        (r'\"Casting\": {\"Method\": \"N/A\", \"Temperature\": \"N/A\", \"Diameter\": \"N/A\"}', ''),        \n",
    "        (r'\"Austenitization\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Transformation\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),        \n",
    "        (r'\"Homogenization\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"},', ''),\n",
    "        (r'\"Solution\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\", \"HeatingRate\": \"N/A\"}', ''),        \n",
    "        (r'\"Isothermal Aging\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Pre-Aging\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),        \n",
    "        (r'\"Retrogression\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Re-aging\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),            \n",
    "        (r'\"Natural Aging\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\"}', ''),\n",
    "        (r'\"Creep Aging\": {\"Temperature\": \"N/A\", \"Time\": \"N/A\", \"Stress\": \"N/A\"}', ''), \n",
    "        (r'\"Melting\": {\"Method\": \"{N/A}\",\"Equipment\": \"{N/A}\",\"Temperature\": \"{N/A}\", \"Atmosphere\": \"{N/A}\"}', ''),\n",
    "        (r'\"Casting\": {\"Method\": \"{N/A}\",\"Temperature\": \"{N/A}\", \"Diameter\": \"{N/A}\"}', ''), \n",
    "        (r'\"Austenitization\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Transformation\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''), \n",
    "        (r'\"Homogenization\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"},', ''),\n",
    "        (r'\"Solution\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\", \"HeatingRate\": \"{N/A}\"}', ''), \n",
    "        (r'\"Isothermal Aging\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Pre-Aging\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''), \n",
    "        (r'\"Retrogression\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Re-aging\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''), \n",
    "        (r'\"Natural Aging\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Creep Aging\": {\"Temperature\": \"{{N/A}}\", \"Time\": \"{{N/A}}\", \"Stress\": \"{N/A}\"}', ''),     \n",
    "        (r'\"Melting\": N/A', ''),\n",
    "        (r'\"Casting\": N/A', ''),        \n",
    "        (r'\"Austenitization\": N/A', ''),\n",
    "        (r'\"Transformation\": N/A', ''),        \n",
    "        (r'\"Homogenization\": N/A', ''), \n",
    "        (r'\"Solution\": N/A', ''),         \n",
    "        (r'\"Isothermal Aging\": N/A', ''), \n",
    "        (r'\"Pre-Aging\": N/A', ''),         \n",
    "        (r'\"Retrogression\": N/A', ''), \n",
    "        (r'\"Re-aging\": N/A', ''),             \n",
    "        (r'\"Natural Aging\": N/A', ''), \n",
    "        (r'\"Creep Aging\": N/A', ''),  \n",
    "        (r'\"Melting\": N/A', ''), \n",
    "        (r'\"Casting\": N/A', ''),  \n",
    "        (r'\"Austenitization\": N/A', ''), \n",
    "        (r'\"Transformation\": N/A', ''),  \n",
    "        (r'\"Homogenization\": N/A', ''), \n",
    "        (r'\"Solution\": N/A', ''),  \n",
    "        (r'\"Isothermal Aging\": N/A', ''), \n",
    "        (r'\"Pre-Aging\": N/A', ''),  \n",
    "        (r'\"Retrogression\": N/A', ''), \n",
    "        (r'\"Re-aging\": N/A', ''),  \n",
    "        (r'\"Natural Aging\": N/A', ''), \n",
    "        (r'\"Creep Aging\": N/A', ''),    \n",
    "        (r'\"Temperature\": \"{N/A}\"', ''),\n",
    "        (r'\"Time\": \"{N/A}\"', ''), \n",
    "        (r'\"Rotation speed\": \"N/A\"', ''), \n",
    "        (r'\"Processing speed\": \"N/A\"', ''), \n",
    "        (r'\"Laser Processing\": \"N/A\"', ''), \n",
    "        (r'\"Laser Power\": \"N/A\"', ''), \n",
    "        (r'\"Rotation speed\": \"N/A\"', ''),     \n",
    "        (r'\"Natural Aging\": {\"Temperature\": \"{N/A}\", \"Time\": \"{N/A}\"}', ''),\n",
    "        (r'\"Stress\": \"{N/A}\"', ''),                       \n",
    "        (r'\"Alloy Formula\":', ''),\n",
    "        (r'\"States\":', ''),\n",
    "        (r'\"Composition\":', ''),\n",
    "        (r'\"Sample Name\":', ''),\n",
    "        (r'\"Alloy\":', ''),\n",
    "        (r'\"Method\":', ''),\n",
    "        (r'\"Name\":', ''),\n",
    "        (r'\"Temperature\":', ''),  \n",
    "        (r'\"Cooling\": {\"N/A\"}', ''),\n",
    "        (r'\"Quenching\": {\"N/A\"}', ''),                                                      \n",
    "        (r'\"Time\":', ''),\n",
    "        (r'\"HeatingRate\":', ''),\n",
    "        (r'\"Diameter\":', ''),\n",
    "        (r'\"Equipment\":', ''),\n",
    "        (r'\"Atmosphere\":', ''),\n",
    "        (r'\"Diameter\":', ''),\n",
    "        (r'\"Stress\":', ''),\n",
    "        (r'\"Heat treatment\":', ''),\n",
    "        (r'\"Other Deformation\":', ''),\n",
    "        (r'\"Other Manufacturing/Preparation Processing\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),\n",
    "        (r'\"Strengthening Mechanism\":', ''),\n",
    "        (r'\"Strengthening Phase\":', ''),\n",
    "        (r'\"Strengthening mechanism and proportion\":', ''),\n",
    "        (r'\"Strengthening Effect\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),\n",
    "        (r'\"Heat_Treatment\":', ''),        \n",
    "        (r'\"Other Parameters\":', ''),\n",
    "        (r'\"{Casting}\"', ''),  # Deformation/Manufacturing_Technique\n",
    "        (r'\"{Solution-treated}\"', ''), # Deformation/Manufacturing_Technique\n",
    "        (r'\"{Water Quenching}\"', ''),  # Deformation/Manufacturing_Technique\n",
    "        (r'\"{Aging/Ageing}\"', ''),  # Deformation/Manufacturing_Technique\n",
    "        (r'\"{Homogenizing}\"', ''),        \n",
    "        (r'\\{.*? cooling\\}', ''),\n",
    "        (r'\\{.*? Cooling\\}', ''),\n",
    "        (r'\\{.*? quenching\\}', ''),\n",
    "        (r'\\{.*? Quenching\\}', ''),\n",
    "        (r'\"Microstructure\":', ''),\n",
    "        (r'\"Description\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Description\":\"N/A\",?', ''),\n",
    "        (r'\"Description\": \"N/A\"', ''),\n",
    "        (r'\"Description\":\"[^\"]*\",?', ''),\n",
    "        (r'\"Precipitate\":', ''),   \n",
    "        (r'\"Solute-Atoms\":', ''),\n",
    "        (r'\"Grain Boundary\":', ''),   \n",
    "        (r'\"Grain Characteristics\":', ''),\n",
    "        (r'\"Dislocation\":', ''),   \n",
    "        (r'\"Defect\":', ''),\n",
    "        (r'\"Texture\":', ''),   \n",
    "        (r'\"Segregation\":', ''), \n",
    "        (r'\"Zones and Deformation Bands\":', ''),   \n",
    "        (r'\"Fracture\":', ''),\n",
    "        (r'\"Crack\":', ''),\n",
    "        (r'\"Description\": \"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Description\": \"N/A\",?', ''),\n",
    "        (r'\"Description\": \"[^\"]*\",?', ''),\n",
    "        (r'\"Characteristic\":\"\\{.*?\\}\",?', ''),\n",
    "        (r'\"Characteristic\":\"N/A\",?', ''),\n",
    "        (r'\"Characteristic\":\"[^\"]*\",?', ''),\n",
    "        (r'\"States\":\"{N/A}\"', ''),\n",
    "        (r'\"Mixing Enthalpy\":\"{N/A}\"', ''),\n",
    "        (r'\"Mixing Entropy\":\"{N/A}\"', ''),\n",
    "        (r'\"Heat Capacity\":\"{N/A}\"', ''),\n",
    "        (r'\"Thermal conductivity\":\"{N/A}\"', ''),\n",
    "        (r'\"Thermal expansion\":\"{N/A}\"', ''),\n",
    "        (r'\"Thermal diffusivity\":\"{N/A}\"', ''),\n",
    "        (r'\"Curie Temperature\": \"{N/A}\"', ''),\n",
    "        (r'\"Transformation temperature\":\"{N/A}\"', ''),\n",
    "        (r'\"Characteristic\":\"[^\"]*\",?', ''),\n",
    "        (r'\"Characteristic\":\"[^\"]*\",?', ''),\n",
    "        (r'json', ''),\n",
    "        (r'\"\\d+\"', ''),\n",
    "        (r':', ' '),\n",
    "        (r'\\\\', ' '),\n",
    "        (r'\"Other Properties\"', ' '),\n",
    "        (r',', ' ')          \n",
    "    ],\n",
    "}\n",
    "###  *****   Config *****\n",
    "\n",
    "# Function to extract json block\n",
    "def extract_json_block(text):\n",
    "    \"\"\"\n",
    "    Extracts JSON content between ```{ and }``` markers.\n",
    "    Returns the content without the markers.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    pattern = config[\"json_block_pattern\"]\n",
    "    match = re.search(pattern, text, re.DOTALL)  # re.DOTALL allows . to match newlines\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "# Function to calculate metrics for a single row\n",
    "def calculate_metrics(row):\n",
    "    def tokenize(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return []\n",
    "        tokens = []\n",
    "        for item in re.split(r'[:,]', text.strip()):\n",
    "            tokens.extend([t.strip() for t in item.split() if t.strip()])\n",
    "        return tokens\n",
    "\n",
    "    groundtruth_tokens = tokenize(row[config[\"output_columns\"][\"groundtruth_column\"]])\n",
    "    extracted_tokens = tokenize(row[config[\"output_columns\"][\"extracted_column\"]])\n",
    "\n",
    "    # If both are empty, return perfect metrics\n",
    "    if not groundtruth_tokens and not extracted_tokens:\n",
    "        return 1, 0, 0  # TP, FP, FN\n",
    "\n",
    "    true_positives = 0\n",
    "    used_positions = set()\n",
    "\n",
    "    for i, gt_token in enumerate(groundtruth_tokens):\n",
    "        for j, ex_token in enumerate(extracted_tokens):\n",
    "            if j not in used_positions and gt_token == ex_token:\n",
    "                true_positives += 1\n",
    "                used_positions.add(j)\n",
    "                break\n",
    "\n",
    "    false_positives = len(extracted_tokens) - true_positives\n",
    "    false_negatives = len(groundtruth_tokens) - true_positives\n",
    "\n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "def calculate_char_metrics(row):\n",
    "    # Get strings\n",
    "    groundtruth = str(row[config[\"output_columns\"][\"groundtruth_column\"]]).strip()\n",
    "    extracted = str(row[config[\"output_columns\"][\"extracted_column\"]]).strip()\n",
    "    \n",
    "    # If both are empty, return perfect metrics\n",
    "    if not groundtruth and not extracted:\n",
    "        return 1, 0, 0  # TP, FP, FN\n",
    "    \n",
    "    # Create character count dictionaries\n",
    "    gt_chars = {}\n",
    "    ex_chars = {}\n",
    "    \n",
    "    # Count occurrences of each character (ignoring whitespace and punctuation)\n",
    "    for char in groundtruth:\n",
    "        if char.strip() and not char in ',./:;(){}[]':\n",
    "            gt_chars[char] = gt_chars.get(char, 0) + 1\n",
    "            \n",
    "    for char in extracted:\n",
    "        if char.strip() and not char in ',./:;(){}[]':\n",
    "            ex_chars[char] = ex_chars.get(char, 0) + 1\n",
    "    \n",
    "    # Calculate true positives\n",
    "    true_positives = 0\n",
    "    for char, count in gt_chars.items():\n",
    "        if char in ex_chars:\n",
    "            true_positives += min(count, ex_chars[char])\n",
    "    \n",
    "    # Calculate false positives and false negatives\n",
    "    false_positives = sum(ex_chars.values()) - true_positives\n",
    "    false_negatives = sum(gt_chars.values()) - true_positives\n",
    "    \n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "# Function to calculate metrics for all rows\n",
    "def calculate_row_metrics(df):\n",
    "    results = []\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    total_char_tp, total_char_fp, total_char_fn = 0, 0, 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Token-level metrics\n",
    "        tp, fp, fn = calculate_metrics(row)\n",
    "        # Character-level metrics\n",
    "        char_tp, char_fp, char_fn = calculate_char_metrics(row)\n",
    "        \n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        total_char_tp += char_tp\n",
    "        total_char_fp += char_fp\n",
    "        total_char_fn += char_fn\n",
    "\n",
    "        # Calculate token-level metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "\n",
    "        # Calculate character-level metrics\n",
    "        char_precision = char_tp / (char_tp + char_fp) if (char_tp + char_fp) > 0 else 0\n",
    "        char_recall = char_tp / (char_tp + char_fn) if (char_tp + char_fn) > 0 else 0\n",
    "        char_f1 = (2 * char_precision * char_recall) / (char_precision + char_recall) if (char_precision + char_recall) > 0 else 0\n",
    "        char_accuracy = char_tp / (char_tp + char_fp + char_fn) if (char_tp + char_fp + char_fn) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            config[\"output_columns\"][\"id_column\"]: row[config[\"output_columns\"][\"id_column\"]],\n",
    "            \"GroundTruth\": row[config[\"output_columns\"][\"groundtruth_column\"]],\n",
    "            \"Extracted\": row[config[\"output_columns\"][\"extracted_column\"]],\n",
    "            \"Token_Precision\": precision,\n",
    "            \"Token_Recall\": recall,\n",
    "            \"Token_F1\": f1,\n",
    "            \"Token_Accuracy\": accuracy,\n",
    "            \"Token_TP\": tp,\n",
    "            \"Token_FP\": fp,\n",
    "            \"Token_FN\": fn,\n",
    "            \"Char_Precision\": char_precision,\n",
    "            \"Char_Recall\": char_recall,\n",
    "            \"Char_F1\": char_f1,\n",
    "            \"Char_Accuracy\": char_accuracy,\n",
    "            \"Char_TP\": char_tp,\n",
    "            \"Char_FP\": char_fp,\n",
    "            \"Char_FN\": char_fn\n",
    "        })\n",
    "\n",
    "    # Calculate overall token-level metrics\n",
    "    total_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    total_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    total_f1 = (2 * total_precision * total_recall) / (total_precision + total_recall) if (total_precision + total_recall) > 0 else 0\n",
    "    total_accuracy = total_tp / (total_tp + total_fp + total_fn) if (total_tp + total_fp + total_fn) > 0 else 0\n",
    "\n",
    "    # Calculate overall character-level metrics\n",
    "    total_char_precision = total_char_tp / (total_char_tp + total_char_fp) if (total_char_tp + total_char_fp) > 0 else 0\n",
    "    total_char_recall = total_char_tp / (total_char_tp + total_char_fn) if (total_char_tp + total_char_fn) > 0 else 0\n",
    "    total_char_f1 = (2 * total_char_precision * total_char_recall) / (total_char_precision + total_char_recall) if (total_char_precision + total_char_recall) > 0 else 0\n",
    "    total_char_accuracy = total_char_tp / (total_char_tp + total_char_fp + total_char_fn) if (total_char_tp + total_char_fp + total_char_fn) > 0 else 0\n",
    "\n",
    "    total_metrics = {\n",
    "        \"Token_Precision\": total_precision,\n",
    "        \"Token_Recall\": total_recall,\n",
    "        \"Token_F1\": total_f1,\n",
    "        \"Token_Accuracy\": total_accuracy,\n",
    "        \"Token_TP\": total_tp,\n",
    "        \"Token_FP\": total_fp,\n",
    "        \"Token_FN\": total_fn,\n",
    "        \"Char_Precision\": total_char_precision,\n",
    "        \"Char_Recall\": total_char_recall,\n",
    "        \"Char_F1\": total_char_f1,\n",
    "        \"Char_Accuracy\": total_char_accuracy,\n",
    "        \"Char_TP\": total_char_tp,\n",
    "        \"Char_FP\": total_char_fp,\n",
    "        \"Char_FN\": total_char_fn\n",
    "    }\n",
    "    \n",
    "    return results, total_metrics\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Extract GroundTruth data\n",
    "    rows = []\n",
    "    with open(config[\"json_file_path\"], 'r', encoding='utf-8') as infile:\n",
    "        current_row = {config[\"output_columns\"][\"id_column\"]: \"\", config[\"output_columns\"][\"groundtruth_column\"]: \"\"}\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check for ID line\n",
    "            if line.startswith(config[\"line_starts_with\"][\"id\"]):\n",
    "                id_match = re.search(config[\"id_pattern\"], line)\n",
    "                if id_match:\n",
    "                    current_row[config[\"output_columns\"][\"id_column\"]] = id_match.group(1)\n",
    "\n",
    "            # Check for GroundTruth composition line\n",
    "            elif line.startswith(config[\"line_starts_with\"][\"groundtruth\"]):\n",
    "                composition_match = re.search(config[\"json_extraction_pattern\"], line)\n",
    "                if composition_match:\n",
    "                    current_row[config[\"output_columns\"][\"groundtruth_column\"]] = composition_match.group(1)\n",
    "                rows.append(current_row)\n",
    "                current_row = {config[\"output_columns\"][\"id_column\"]: \"\", config[\"output_columns\"][\"groundtruth_column\"]: \"\"}\n",
    "\n",
    "    groundtruth_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Load extracted data from CSV\n",
    "    extracted_df = pd.read_csv(config[\"csv_file_path\"])\n",
    "    if config[\"output_columns\"][\"csv_column\"] in extracted_df.columns:\n",
    "        groundtruth_df[config[\"output_columns\"][\"extracted_column\"]] = extracted_df[config[\"output_columns\"][\"csv_column\"]]\n",
    "    else:\n",
    "        raise ValueError(\"The input CSV file must contain the specified column!\")\n",
    "\n",
    "    # Preprocess content\n",
    "    def preprocess(content):\n",
    "        if not isinstance(content, str):\n",
    "            return content\n",
    "        \n",
    "        # First extract any JSON blocks\n",
    "        content = extract_json_block(content)\n",
    "        \n",
    "        for pattern, replacement in config[\"preprocess_patterns\"]:\n",
    "            content = re.sub(pattern, replacement, content)\n",
    "        return content.strip().replace('\"', '').replace('{', '').replace('}', '')\n",
    "\n",
    "    groundtruth_df[config[\"output_columns\"][\"groundtruth_column\"]] = groundtruth_df[config[\"output_columns\"][\"groundtruth_column\"]].apply(preprocess)\n",
    "    groundtruth_df[config[\"output_columns\"][\"extracted_column\"]] = groundtruth_df[config[\"output_columns\"][\"extracted_column\"]].apply(preprocess)\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    comparison_df = groundtruth_df[\n",
    "        [\n",
    "            config[\"output_columns\"][\"id_column\"],\n",
    "            config[\"output_columns\"][\"groundtruth_column\"],\n",
    "            config[\"output_columns\"][\"extracted_column\"]\n",
    "        ]\n",
    "    ]\n",
    "    comparison_df.to_csv(config[\"output_csv_path\"], index=False)\n",
    "\n",
    "    # Calculate metrics\n",
    "    results, total_metrics = calculate_row_metrics(groundtruth_df)\n",
    "    \n",
    "    # Print results for each row\n",
    "    # print(\"Results for each row:\")\n",
    "    # for result in results:\n",
    "    #     print(f\"ID: {result[config['output_columns']['id_column']]}\")\n",
    "    #     print(f\"GroundTruth: {result['GroundTruth']}\")\n",
    "    #     print(f\"Extracted: {result['Extracted']}\")\n",
    "    #     print(f\"Token Precision: {result['Token_Precision']:.4f}\")\n",
    "    #     print(f\"Token Recall: {result['Token_Recall']:.4f}\")\n",
    "    #     print(f\"Token F1: {result['Token_F1']:.4f}\")\n",
    "    #     print(f\"Token Accuracy: {result['Token_Accuracy']:.4f}\")\n",
    "    #     print(f\"Token TP: {result['Token_TP']}, FP: {result['Token_FP']}, FN: {result['Token_FN']}\")\n",
    "    #     print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(\"Token-level Metrics:\")\n",
    "    print(f\"Token Precision: {total_metrics['Token_Precision']:.4f}\")\n",
    "    print(f\"Token Recall: {total_metrics['Token_Recall']:.4f}\")\n",
    "    print(f\"Token F1-Score: {total_metrics['Token_F1']:.4f}\")\n",
    "    print(f\"Token Accuracy: {total_metrics['Token_Accuracy']:.4f}\")\n",
    "    print(f\"Token TP: {total_metrics['Token_TP']}, FP: {total_metrics['Token_FP']}, FN: {total_metrics['Token_FN']}\")\n",
    "\n",
    "    print(\"\\nCharacter-level Metrics:\")\n",
    "    print(f\"Char Precision: {total_metrics['Char_Precision']:.4f}\")\n",
    "    print(f\"Char Recall: {total_metrics['Char_Recall']:.4f}\")\n",
    "    print(f\"Char F1-Score: {total_metrics['Char_F1']:.4f}\")\n",
    "    print(f\"Char Accuracy: {total_metrics['Char_Accuracy']:.4f}\")\n",
    "    print(f\"Char TP: {total_metrics['Char_TP']}, FP: {total_metrics['Char_FP']}, FN: {total_metrics['Char_FN']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "### ***** Config *****\n",
    "config = {\n",
    "    # 文件路径配置\n",
    "    \"json_dir1\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/GroundTruth/Table/GroundTruth\",\n",
    "    \"json_dir2\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Results/Table/SparkMax\",\n",
    "    \"csv_output_path\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Results/Table/output.csv\",\n",
    "    # 列名配置\n",
    "    \"output_columns\": {\n",
    "        \"filename1_column\": \"GroundTruth-DOI\",\n",
    "        \"content1_column\": \"GroundTruth\",\n",
    "        \"filename2_column\": \"Extraction-DOI\",\n",
    "        \"content2_column\": \"Extraction\"\n",
    "    },\n",
    "    # 需要替换的内容模式\n",
    "    \"preprocess_patterns\": [\n",
    "        (r'Here is the JSON output.*?(format|input)', ''),  # 去掉 \"Here is the JSON output\" 开头到 \"format\" 为止的部分\n",
    "        (r'Here is the output.*?(format|input)', ''),\n",
    "        (r'Here is.*?(format|input)', ''),\n",
    "        (r'/', ''),  # 去掉所有单斜杠 `/`\n",
    "        (r'\\\\', ''),  # 去掉所有反斜杠 `\\`\n",
    "        (r'{', ''),  # 去掉左花括号 `{`\n",
    "        (r'}', ''),  # 去掉右花括号 `}`\n",
    "        (r'```json', ''),  # 替换 ```json 为空\n",
    "        (r'.json', ''),  # 替换 .json 为空\n",
    "        (r'```', ''),  # 替换 ``` 为空\n",
    "        (r'\\\\n', ''),  # 替换 `\\n` 为空\n",
    "        (r'\\n', ''),  # 替换换行符为空\n",
    "        (r'/n', ''),  # 替换换行符为空        \n",
    "        (r'nnn', ''),  # 替换换行符为空\n",
    "        (r'nn', ''),  # 替换换行符为空\n",
    "        (r'n###', '###'),  # 替换换行符为空\n",
    "        (r':', ''),  # 替换冒号为空\n",
    "        (r',', ''),  # 替换冒号为空\n",
    "        (r\"'\", ''), # 替换单引号为空                                    \n",
    "        (r'\"', '')  # 替换双引号为空\n",
    "    ],\n",
    "    # 需要替换的内容模式（仅用于第二列）\n",
    "    \"preprocess_patterns2\": [\n",
    "        (r'###<[^>]+>', '###<')  # 替换所有 ###<...> 中第一个尖括号里的内容\n",
    "    ],\n",
    "    # 需要替换的内容模式（用于第二列和第四列）\n",
    "    \"preprocess_patterns3\": [\n",
    "        (r'###', ' '),  # 替换 ### 为空格\n",
    "        (r'<', ' '),  # 替换 < 为空格\n",
    "        (r'=', ' '),  # 替换 = 为空格\n",
    "        (r'>', ' ')  # 替换 > 为空格\n",
    "    ],\n",
    "    # JSON 块提取模式\n",
    "    \"json_block_pattern\": r'```\\{([\\s\\S]*?)\\}```'\n",
    "}\n",
    "### ***** Config *****\n",
    "\n",
    "# 处理JSON内容，应用所有替换模式\n",
    "def process_content(content, patterns):\n",
    "    if not isinstance(content, str):\n",
    "        return content\n",
    "    # 应用所有预处理模式\n",
    "    for pattern, replacement in patterns:\n",
    "        content = re.sub(pattern, replacement, content)\n",
    "    return content\n",
    "\n",
    "# 提取 JSON 块\n",
    "def extract_json_block(text):\n",
    "    \"\"\"\n",
    "    Extracts JSON content between ```{ and }``` markers.\n",
    "    Returns the content without the markers.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    pattern = config[\"json_block_pattern\"]\n",
    "    match = re.search(pattern, text, re.DOTALL)  # re.DOTALL allows . to match newlines\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "# 计算 Token Level 的 TP、FP、FN\n",
    "def calculate_metrics(row):\n",
    "    def tokenize(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return []\n",
    "        tokens = []\n",
    "        for item in re.split(r'[:,]', text.strip()):\n",
    "            tokens.extend([t.strip() for t in item.split() if t.strip()])\n",
    "        return tokens\n",
    "\n",
    "    groundtruth_tokens = tokenize(row[config[\"output_columns\"][\"content1_column\"]])\n",
    "    extracted_tokens = tokenize(row[config[\"output_columns\"][\"content2_column\"]])\n",
    "\n",
    "    # If both are empty, return perfect metrics\n",
    "    if not groundtruth_tokens and not extracted_tokens:\n",
    "        return 1, 0, 0  # TP, FP, FN\n",
    "\n",
    "    true_positives = 0\n",
    "    used_positions = set()\n",
    "\n",
    "    for i, gt_token in enumerate(groundtruth_tokens):\n",
    "        for j, ex_token in enumerate(extracted_tokens):\n",
    "            if j not in used_positions and gt_token == ex_token:\n",
    "                true_positives += 1\n",
    "                used_positions.add(j)\n",
    "                break\n",
    "\n",
    "    false_positives = len(extracted_tokens) - true_positives\n",
    "    false_negatives = len(groundtruth_tokens) - true_positives\n",
    "\n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "# 计算 Character Level 的 TP、FP、FN\n",
    "def calculate_char_metrics(row):\n",
    "    # Get strings\n",
    "    groundtruth = str(row[config[\"output_columns\"][\"content1_column\"]]).strip()\n",
    "    extracted = str(row[config[\"output_columns\"][\"content2_column\"]]).strip()\n",
    "    \n",
    "    # If both are empty, return perfect metrics\n",
    "    if not groundtruth and not extracted:\n",
    "        return 1, 0, 0  # TP, FP, FN\n",
    "    \n",
    "    # Create character count dictionaries\n",
    "    gt_chars = {}\n",
    "    ex_chars = {}\n",
    "    \n",
    "    # Count occurrences of each character (ignoring whitespace and punctuation)\n",
    "    for char in groundtruth:\n",
    "        if char.strip() and not char in ',./:;(){}[]':\n",
    "            gt_chars[char] = gt_chars.get(char, 0) + 1\n",
    "            \n",
    "    for char in extracted:\n",
    "        if char.strip() and not char in ',./:;(){}[]':\n",
    "            ex_chars[char] = ex_chars.get(char, 0) + 1\n",
    "    \n",
    "    # Calculate true positives\n",
    "    true_positives = 0\n",
    "    for char, count in gt_chars.items():\n",
    "        if char in ex_chars:\n",
    "            true_positives += min(count, ex_chars[char])\n",
    "    \n",
    "    # Calculate false positives and false negatives\n",
    "    false_positives = sum(ex_chars.values()) - true_positives\n",
    "    false_negatives = sum(gt_chars.values()) - true_positives\n",
    "    \n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "# 计算整体指标\n",
    "def calculate_overall_metrics(df):\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    total_char_tp, total_char_fp, total_char_fn = 0, 0, 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Token-level metrics\n",
    "        tp, fp, fn = calculate_metrics(row)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        # Character-level metrics\n",
    "        char_tp, char_fp, char_fn = calculate_char_metrics(row)\n",
    "        total_char_tp += char_tp\n",
    "        total_char_fp += char_fp\n",
    "        total_char_fn += char_fn\n",
    "\n",
    "    # Calculate overall token-level metrics\n",
    "    token_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    token_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    token_f1 = (2 * token_precision * token_recall) / (token_precision + token_recall) if (token_precision + token_recall) > 0 else 0\n",
    "    token_accuracy = total_tp / (total_tp + total_fp + total_fn) if (total_tp + total_fp + total_fn) > 0 else 0\n",
    "\n",
    "    # Calculate overall character-level metrics\n",
    "    char_precision = total_char_tp / (total_char_tp + total_char_fp) if (total_char_tp + total_char_fp) > 0 else 0\n",
    "    char_recall = total_char_tp / (total_char_tp + total_char_fn) if (total_char_tp + total_char_fn) > 0 else 0\n",
    "    char_f1 = (2 * char_precision * char_recall) / (char_precision + char_recall) if (char_precision + char_recall) > 0 else 0\n",
    "    char_accuracy = total_char_tp / (total_char_tp + total_char_fp + total_char_fn) if (total_char_tp + total_char_fp + total_char_fn) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Token_Precision\": token_precision,\n",
    "        \"Token_Recall\": token_recall,\n",
    "        \"Token_F1\": token_f1,\n",
    "        \"Token_Accuracy\": token_accuracy,\n",
    "        \"Token_TP\": total_tp,\n",
    "        \"Token_FP\": total_fp,\n",
    "        \"Token_FN\": total_fn,\n",
    "        \"Char_Precision\": char_precision,\n",
    "        \"Char_Recall\": char_recall,\n",
    "        \"Char_F1\": char_f1,\n",
    "        \"Char_Accuracy\": char_accuracy,\n",
    "        \"Char_TP\": total_char_tp,\n",
    "        \"Char_FP\": total_char_fp,\n",
    "        \"Char_FN\": total_char_fn\n",
    "    }\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 从第一个目录读取数据\n",
    "    data = []\n",
    "    for filename in os.listdir(config[\"json_dir1\"]):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(config[\"json_dir1\"], filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # 解析 JSON 内容，确保 Unicode 转义字符被正确解码\n",
    "                try:\n",
    "                    content = json.loads(content)  # 解析 JSON 内容\n",
    "                    content = json.dumps(content, ensure_ascii=False)  # 将 JSON 转换为字符串，确保 Unicode 字符被正确显示\n",
    "                except json.JSONDecodeError:\n",
    "                    # 如果 JSON 解析失败，直接使用原始内容\n",
    "                    pass\n",
    "                # 应用通用替换模式\n",
    "                content = process_content(content, config[\"preprocess_patterns\"])  # 应用通用替换模式\n",
    "                # 去掉文件名中的 .json 后缀\n",
    "                filename_without_ext = os.path.splitext(filename)[0]\n",
    "                # 创建只有两列的行\n",
    "                row = [filename_without_ext, content, \"\", \"\"]\n",
    "                data.append(row)\n",
    "\n",
    "    # 创建包含四列的 DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        config[\"output_columns\"][\"filename1_column\"],\n",
    "        config[\"output_columns\"][\"content1_column\"],\n",
    "        config[\"output_columns\"][\"filename2_column\"],\n",
    "        config[\"output_columns\"][\"content2_column\"]\n",
    "    ])\n",
    "\n",
    "    # 从第二个目录读取数据填充第三和第四列\n",
    "    row_index = 0\n",
    "    for filename in os.listdir(config[\"json_dir2\"]):\n",
    "        if filename.endswith(\".json\") and row_index < len(df):\n",
    "            file_path = os.path.join(config[\"json_dir2\"], filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # 解析 JSON 内容，确保 Unicode 转义字符被正确解码\n",
    "                try:\n",
    "                    content = json.loads(content)  # 解析 JSON 内容\n",
    "                    content = json.dumps(content, ensure_ascii=False)  # 将 JSON 转换为字符串，确保 Unicode 字符被正确显示\n",
    "                except json.JSONDecodeError:\n",
    "                    # 如果 JSON 解析失败，直接使用原始内容\n",
    "                    pass\n",
    "                # 应用通用替换模式\n",
    "                content = process_content(content, config[\"preprocess_patterns\"])  # 应用通用替换模式\n",
    "                # 去掉文件名中的 .json 后缀\n",
    "                filename_without_ext = os.path.splitext(filename)[0]\n",
    "                # 更新当前行的第三和第四列\n",
    "                df.at[row_index, config[\"output_columns\"][\"filename2_column\"]] = filename_without_ext\n",
    "                df.at[row_index, config[\"output_columns\"][\"content2_column\"]] = content\n",
    "                row_index += 1\n",
    "\n",
    "    # 对第二列应用额外的替换规则\n",
    "    df[config[\"output_columns\"][\"content1_column\"]] = df[config[\"output_columns\"][\"content1_column\"]].apply(\n",
    "        lambda x: process_content(x, config[\"preprocess_patterns2\"])  # 先应用第二列专用规则\n",
    "    )\n",
    "    df[config[\"output_columns\"][\"content1_column\"]] = df[config[\"output_columns\"][\"content1_column\"]].apply(\n",
    "        lambda x: process_content(x, config[\"preprocess_patterns\"])  # 再应用通用替换模式\n",
    "    )\n",
    "\n",
    "    # 对第二列和第四列应用 preprocess_patterns3\n",
    "    df[config[\"output_columns\"][\"content1_column\"]] = df[config[\"output_columns\"][\"content1_column\"]].apply(\n",
    "        lambda x: process_content(x, config[\"preprocess_patterns3\"])  # 应用 preprocess_patterns3\n",
    "    )\n",
    "    df[config[\"output_columns\"][\"content2_column\"]] = df[config[\"output_columns\"][\"content2_column\"]].apply(\n",
    "        lambda x: process_content(x, config[\"preprocess_patterns3\"])  # 应用 preprocess_patterns3\n",
    "    )\n",
    "\n",
    "    # 计算每一行的 Token Level 和 Character Level 的 TP、FP、FN\n",
    "    df[\"Token_TP\"] = 0\n",
    "    df[\"Token_FP\"] = 0\n",
    "    df[\"Token_FN\"] = 0\n",
    "    df[\"Char_TP\"] = 0\n",
    "    df[\"Char_FP\"] = 0\n",
    "    df[\"Char_FN\"] = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # 计算 Token Level 的 TP、FP、FN\n",
    "        token_tp, token_fp, token_fn = calculate_metrics(row)\n",
    "        df.at[index, \"Token_TP\"] = token_tp\n",
    "        df.at[index, \"Token_FP\"] = token_fp\n",
    "        df.at[index, \"Token_FN\"] = token_fn\n",
    "        \n",
    "        # 计算 Character Level 的 TP、FP、FN\n",
    "        char_tp, char_fp, char_fn = calculate_char_metrics(row)\n",
    "        df.at[index, \"Char_TP\"] = char_tp\n",
    "        df.at[index, \"Char_FP\"] = char_fp\n",
    "        df.at[index, \"Char_FN\"] = char_fn\n",
    "\n",
    "    # 计算整体指标\n",
    "    overall_metrics = calculate_overall_metrics(df)\n",
    "\n",
    "    # 输出整体指标\n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(\"Token-level Metrics:\")\n",
    "    print(f\"Token Precision: {overall_metrics['Token_Precision']:.4f}\")\n",
    "    print(f\"Token Recall: {overall_metrics['Token_Recall']:.4f}\")\n",
    "    print(f\"Token F1-Score: {overall_metrics['Token_F1']:.4f}\")\n",
    "    print(f\"Token Accuracy: {overall_metrics['Token_Accuracy']:.4f}\")\n",
    "    print(f\"Token TP: {overall_metrics['Token_TP']}, FP: {overall_metrics['Token_FP']}, FN: {overall_metrics['Token_FN']}\")\n",
    "\n",
    "    print(\"\\nCharacter-level Metrics:\")\n",
    "    print(f\"Char Precision: {overall_metrics['Char_Precision']:.4f}\")\n",
    "    print(f\"Char Recall: {overall_metrics['Char_Recall']:.4f}\")\n",
    "    print(f\"Char F1-Score: {overall_metrics['Char_F1']:.4f}\")\n",
    "    print(f\"Char Accuracy: {overall_metrics['Char_Accuracy']:.4f}\")\n",
    "    print(f\"Char TP: {overall_metrics['Char_TP']}, FP: {overall_metrics['Char_FP']}, FN: {overall_metrics['Char_FN']}\")\n",
    "\n",
    "    # 保存为 CSV，确保内容被正确引用\n",
    "    df.to_csv(config[\"csv_output_path\"], index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "    print(f\"CSV 文件已保存至 {config['csv_output_path']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstarct Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92 JSON files\n",
      "Successfully processed 92 files\n",
      "Failed to process 0 files\n",
      "\n",
      "First few entries:\n",
      "                          DOI  \\\n",
      "0   10.1007-s00170-018-1931-z   \n",
      "1  10.1007-s00170-019-03789-w   \n",
      "2  10.1007-s00170-020-05072-9   \n",
      "3  10.1007-s00170-020-05242-9   \n",
      "4  10.1007-s00170-023-11237-z   \n",
      "\n",
      "                                      Extracted Info  \n",
      "0  {'Tool_life_increase': {'Uncoated': '54%', 'Ti...  \n",
      "1  Microstructure and micromechanical properties ...  \n",
      "2  {'Initial_density': {'High': 'above 99%', 'Low...  \n",
      "3  {'Ultimate_Tensile_Strength': '315 MPa', 'Micr...  \n",
      "4                   Creep properties at 900°C/80 MPa  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Configurations\n",
    "CONFIG = {\n",
    "    \"folder_path\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Results/Structural-Abstract/GPT-3.5\",\n",
    "    \"output_csv_path\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Results/Structural-Abstract/GPT-3.5.csv\",\n",
    "    \"possible_keys\": [\n",
    "        # \"processing_route\", \"processing route\", \"processingroute\", \"Processing_route\", \"Processing route\", \"ProcessingRoute\", \"PROCESSING_ROUTE\", \"PROCESSING ROUTE\", \"PROCESSINGROUTE\", \"Processing_Route\"\n",
    "        # \"main_idea\", \"main idea\", \"mainidea\", \"Main_idea\", \"Main idea\", \"MainIdea\", \"MAIN_IDEA\", \"MAIN IDEA\", \"MAINIDEA\"\n",
    "        # \"research_object\", \"research object\", \"researchobject\", \"Research_object\", \"Research object\", \"Research_Object\", \"ResearchObject\", \"RESEARCH_OBJECT\", \"RESEARCH OBJECT\", \"RESEARCHOBJECT\"\n",
    "        \"investigated_performance\", \"investigated performance\", \"investigatedperformance\", \"Investigated_performance\", \"Investigated performance\", \"Investigated_Performance\", \"InvestigatedPerformance\", \"INVESTIGATED_PERFORMANCE\", \"INVESTIGATED PERFORMANCE\", \"INVESTIGATEDPERFORMANCE\"\n",
    "         \"mechanical_properties\", \"mechanical properties\", \"mechanicalproperties\", \"Mechanical_properties\", \"Mechanical properties\", \"Mechanical_Properties\", \"MechanicalProperties\", \"MECHANICAL_PROPERTIES\", \"MECHANICAL PROPERTIES\", \"MECHANICALPROPERTIES\"\n",
    "    ],\n",
    "    \n",
    "    \"patterns\": [\n",
    "    # Patterns for values with quotes\n",
    "    # r'\"processing_route\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"processing route\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Processing_route\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Processing route\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"PROCESSING_ROUTE\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"PROCESSING ROUTE\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"processingroute\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"ProcessingRoute\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"PROCESSINGROUTE\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Processing_Route\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Processing_route\"\\s*:\\s*\"([^\"]+)\"',\n",
    "\n",
    "    # r'\"main idea\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Main_idea\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Main idea\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"MAIN_IDEA\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"main_idea\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"MAIN IDEA\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"mainidea\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"MainIdea\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"MAINIDEA\"\\s*:\\s*\"([^\"]+)\"',\n",
    "\n",
    "    # r'\"research_object\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"research object\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Research_object\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Research object\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"RESEARCH_OBJECT\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"RESEARCH OBJECT\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"researchobject\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"ResearchObject\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"RESEARCHOBJECT\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    # r'\"Research_Object\"\\s*:\\s*\"([^\"]+)\"',\n",
    "\n",
    "    r'\"investigated_performance\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"investigated performance\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"Investigated_performance\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"Investigated performance\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"INVESTIGATED_PERFORMANCE\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"INVESTIGATED PERFORMANCE\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"investigatedperformance\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"InvestigatedPerformance\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"INVESTIGATEDPERFORMANCE\"\\s*:\\s*\"([^\"]+)\"',\n",
    "\n",
    "    r'\"mechanical_properties\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"mechanical properties\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"Mechanical_properties\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"Mechanical properties\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"MECHANICAL_PROPERTIES\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"MECHANICAL PROPERTIES\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"mechanicalproperties\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"MechanicalProperties\"\\s*:\\s*\"([^\"]+)\"',\n",
    "    r'\"MECHANICALPROPERTIES\"\\s*:\\s*\"([^\"]+)\"',\n",
    "\n",
    "    # Patterns for values without quotes\n",
    "    # r'\"processing_route\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"processing route\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Processing_route\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Processing route\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"PROCESSING_ROUTE\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"PROCESSING ROUTE\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"processingroute\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"ProcessingRoute\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"PROCESSINGROUTE\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Processing_Route\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Processing_route\"\\s*:\\s*([^\"\\n,]+)',\n",
    "\n",
    "    # r'\"main idea\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Main_idea\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Main idea\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"MAIN_IDEA\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"main_idea\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"MAIN IDEA\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"mainidea\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"MainIdea\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"MAINIDEA\"\\s*:\\s*([^\"\\n,]+)',\n",
    "\n",
    "    # r'\"research_object\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"research object\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Research_object\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Research object\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"RESEARCH_OBJECT\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"RESEARCH OBJECT\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"researchobject\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"ResearchObject\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"RESEARCHOBJECT\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    # r'\"Research_Object\"\\s*:\\s*([^\"\\n,]+)',\n",
    "\n",
    "    r'\"investigated_performance\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"investigated performance\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"Investigated_performance\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"Investigated performance\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"INVESTIGATED_PERFORMANCE\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"INVESTIGATED PERFORMANCE\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"investigatedperformance\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"InvestigatedPerformance\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"INVESTIGATEDPERFORMANCE\"\\s*:\\s*([^\"\\n,]+)',\n",
    "\n",
    "    r'\"mechanical_properties\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"mechanical properties\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"Mechanical_properties\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"Mechanical properties\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"MECHANICAL_PROPERTIES\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"MECHANICAL PROPERTIES\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"mechanicalproperties\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"MechanicalProperties\"\\s*:\\s*([^\"\\n,]+)',\n",
    "    r'\"MECHANICALPROPERTIES\"\\s*:\\s*([^\"\\n,]+)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def extract_fields_from_json(folder_path, output_csv_path, possible_keys, patterns):\n",
    "    \"\"\"\n",
    "    Extract fields from JSON files in a folder and save to CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing JSON files\n",
    "    output_csv_path (str): Path where the CSV file will be saved\n",
    "    possible_keys (list): List of possible keys to extract\n",
    "    patterns (list): List of regex patterns to extract values\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    error_files = []\n",
    "\n",
    "    files = sorted(os.listdir(folder_path))\n",
    "    \n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                extracted_info = extract_fields_from_file(file_path, possible_keys, patterns)\n",
    "                doi = filename.replace('.json', '')\n",
    "                data.append({\"DOI\": doi, \"Extracted Info\": extracted_info})\n",
    "                processed_files += 1\n",
    "            except Exception as e:\n",
    "                error_files.append(filename)\n",
    "                print(f\"Error processing file {filename}: {str(e)}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print(f\"Found {total_files} JSON files\")\n",
    "    print(f\"Successfully processed {processed_files} files\")\n",
    "    print(f\"Failed to process {len(error_files)} files\")\n",
    "    if error_files:\n",
    "        print(\"Files with errors:\")\n",
    "        for err_file in error_files[:10]:\n",
    "            print(f\"  - {err_file}\")\n",
    "        if len(error_files) > 10:\n",
    "            print(f\"  ... and {len(error_files) - 10} more\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_fields_from_file(file_path, possible_keys, patterns):\n",
    "    \"\"\"\n",
    "    Extract fields from a JSON file using multiple methods.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the JSON file\n",
    "    possible_keys (list): List of possible keys to extract\n",
    "    patterns (list): List of regex patterns to extract values\n",
    "    \n",
    "    Returns:\n",
    "    str: Extracted information\n",
    "    \"\"\"\n",
    "    extracted_info = \"\"\n",
    "    \n",
    "    # Method 1: Standard JSON parsing\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            json_data = json.loads(content)\n",
    "            for key in possible_keys:\n",
    "                if key in json_data:\n",
    "                    value = json_data[key]\n",
    "                    if isinstance(value, list):\n",
    "                        extracted_info = \", \".join(value)  # Join list items into a single string\n",
    "                    else:\n",
    "                        extracted_info = value\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Clean content and retry JSON parsing\n",
    "    if not extracted_info:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                clean_content = re.sub(r'```(json)?|```', '', content).strip()\n",
    "                json_data = json.loads(clean_content)\n",
    "                for key in possible_keys:\n",
    "                    if key in json_data:\n",
    "                        value = json_data[key]\n",
    "                        if isinstance(value, list):\n",
    "                            extracted_info = \", \".join(value)\n",
    "                        else:\n",
    "                            extracted_info = value\n",
    "                        break\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Regular expression to directly extract values\n",
    "    if not extracted_info:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                for pattern in patterns:\n",
    "                    match = re.search(pattern, content)\n",
    "                    if match:\n",
    "                        extracted_info = match.group(1).strip()\n",
    "                        break\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = extract_fields_from_json(\n",
    "        CONFIG[\"folder_path\"],\n",
    "        CONFIG[\"output_csv_path\"],\n",
    "        CONFIG[\"possible_keys\"],\n",
    "        CONFIG[\"patterns\"]\n",
    "    )\n",
    "    if not result_df.empty:\n",
    "        print(\"\\nFirst few entries:\")\n",
    "        print(result_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstarct Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "每行的评估指标:\n",
      "--------------------------------------------------------------------------------\n",
      "Excel 文件已保存至 /Users/zixuanzhao/Desktop/Metrics/Structured-Info/Comparsion/Structural Abstract/Investigated_performance/SparkMax.xlsx\n",
      "\n",
      "Overall Metrics:\n",
      "Token-level Metrics:\n",
      "Token Precision: 0.5583\n",
      "Token Recall: 0.8422\n",
      "Token F1-Score: 0.6715\n",
      "Token Accuracy: 0.5054\n",
      "Token TP: 747, FP: 591, FN: 140\n",
      "\n",
      "Character-level Metrics:\n",
      "Char Precision: 0.6183\n",
      "Char Recall: 0.9409\n",
      "Char F1-Score: 0.7462\n",
      "Char Accuracy: 0.5952\n",
      "Char TP: 7037, FP: 4344, FN: 442\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "### ***** Config *****\n",
    "config = {\n",
    "    # 文件路径配置\n",
    "    \"xlsx_path\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Comparsion/Structural Abstract/Investigated_performance.xlsx\",  # 输入的 Excel 文件路径\n",
    "    \"output_xlsx_path\": \"/Users/zixuanzhao/Desktop/Metrics/Structured-Info/Comparsion/Structural Abstract/Investigated_performance/SparkMax.xlsx\",  # 输出的 Excel 文件路径\n",
    "    # 列名配置\n",
    "    \"doi_column\": \"DOI\",  # DOI 列名\n",
    "    \"groundtruth_column\": \"GroundTruth\",  # GroundTruth 列名\n",
    "    \"model_column\": \"SparkMax\",  # 模型列名（可以根据需要修改）\n",
    "    # 需要替换的内容模式\n",
    "    \"preprocess_patterns\": [\n",
    "        (r'Here is the JSON output.*?(format|input)', ''),  # 去掉 \"Here is the JSON output\" 开头到 \"format\" 为止的部分\n",
    "        (r'Here is the output.*?(format|input)', ''),\n",
    "        (r'Here is.*?(format|input)', '')\n",
    "    ],\n",
    "    # 停用词列表（可以根据需要扩展）\n",
    "    \"stop_words\": [\n",
    "        \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"if\", \"then\", \"else\", \"when\", \"at\", \"by\", \"for\", \"in\", \"of\", \"on\", \"to\", \"with\", \"as\", \"about\", \n",
    "        \"after\", \"before\", \"during\", \"until\", \"above\", \"below\", \"from\", \"up\", \"down\", \"out\", \"over\", \"under\", \"again\", \"further\", \"once\", \"here\", \n",
    "        \"there\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \n",
    "        \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"new\", \"newly\", \"newer\", \"newest\", \n",
    "        \"method\", \"methods\", \"methodology\", \"methodological\", \"improving\", \"improve\", \"improves\", \"improved\", \"improvement\", \"improvements\", \n",
    "        \"significant\", \"significance\", \"significantly\", \"proposed\", \"propose\", \"proposes\", \"proposing\", \"proposal\", \"proposals\", \"effect\", \n",
    "        \"effects\", \"effective\", \"effectively\", \"result\", \"results\", \"resulted\", \"resulting\", \"show\", \"shows\", \"showed\", \"showing\", \"find\", \n",
    "        \"finds\", \"found\", \"finding\", \"findings\", \"study\", \"studies\", \"studied\", \"studying\", \"analysis\", \"analyses\", \"analyze\", \"analyzes\", \n",
    "        \"analyzed\", \"analyzing\", \"research\", \"researches\", \"researched\", \"researching\", \"development\", \"developments\", \"develop\", \"develops\", \n",
    "        \"developed\", \"developing\", \"process\", \"processes\", \"processed\", \"processing\", \"approach\", \"approaches\", \"approached\", \"approaching\", \n",
    "        \"technique\", \"techniques\", \"strategy\", \"strategies\", \"system\", \"systems\", \"model\", \"models\", \"framework\", \"frameworks\", \"investigating\", \n",
    "        \"investigate\", \"investigates\", \"investigated\", \"investigation\", \"investigations\", \"revealing\", \"reveal\", \"reveals\", \"revealed\", \n",
    "        \"revelation\", \"influence\", \"influences\", \"influenced\", \"influencing\", \"exhibits\", \"exhibit\", \"exhibited\", \"exhibiting\", \"exhibition\", \n",
    "        \"this\", \"demonstrates\", \"demonstrate\", \"demonstrated\", \"demonstrating\", \"demonstration\", \"indicates\", \"indicate\", \"indicated\", \n",
    "        \"indicating\", \"indication\", \"suggests\", \"suggest\", \"suggested\", \"suggesting\", \"suggestion\", \"observes\", \"observe\", \"observed\", \n",
    "        \"observing\", \"observation\", \"highlights\", \"highlight\", \"highlighted\", \"highlighting\", \"confirms\", \"confirm\", \"confirmed\", \"confirming\", \n",
    "        \"confirmation\", \"proves\", \"prove\", \"proved\", \"proving\", \"proof\", \"explores\", \"explore\", \"explored\", \"exploring\", \"exploration\", \n",
    "        \"discusses\", \"discuss\", \"discussed\", \"discussing\", \"discussion\", \"examines\", \"examine\", \"examined\", \"examining\", \"examination\", \n",
    "        \"evaluates\", \"evaluate\", \"evaluated\", \"evaluating\", \"evaluation\", \"assesses\", \"assess\", \"assessed\", \"assessing\", \"assessment\", \"reports\", \n",
    "        \"report\", \"reported\", \"reporting\", \"presents\", \"present\", \"presented\", \"presenting\", \"presentation\", \"aimed\", \"aim\", \"aims\", \"aiming\", \n",
    "        \"used\", \"use\", \"uses\", \"using\", \"usage\", \"applying\", \"apply\", \"applies\", \"applied\", \"application\", \"focusing\", \"focus\", \"focuses\", \n",
    "        \"focused\", \"role\", \"roles\", \"introduces\", \"introduce\", \"introduced\", \"introducing\", \"introduction\", \"that\", \"designed\", \"design\", \n",
    "        \"designs\", \"designing\", \"good\", \"well\", \"better\", \"best\", \"achieving\", \"achieve\", \"achieves\", \"achieved\", \"achievement\", \"while\", \n",
    "        \"maintaining\", \"maintain\", \"maintains\", \"maintained\", \"maintenance\", \"through\", \"noting\", \"note\", \"notes\", \"noted\", \"due\", \"whereas\", \n",
    "        \"are\", \"is\", \"was\", \"were\", \"be\", \"being\", \"been\", \"affect\", \"affects\", \"affected\", \"affecting\", \"critical\", \"critically\", \"changes\", \n",
    "        \"change\", \"changed\", \"changing\", \"its\", \"adding\", \"add\", \"adds\", \"added\", \"addition\", \"via\", \"lead to\", \"leads to\", \"led to\", \"resulting in\",\n",
    "        \"explain\", \"explains\", \"explained\", \"explaining\", \"explanation\", \"cause\", \"causes\", \"caused\", \"causing\", \"utilize\", \"utilizes\", \"utilized\", \"utilizing\"\n",
    "    ]\n",
    "}\n",
    "### ***** Config *****\n",
    "\n",
    "def process_content(content, patterns):\n",
    "    if not isinstance(content, str):\n",
    "        return content\n",
    "    # 应用所有预处理模式\n",
    "    for pattern, replacement in patterns:\n",
    "        content = re.sub(pattern, replacement, content)\n",
    "    return content\n",
    "\n",
    "def remove_stopwords_and_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # 去除标点符号\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 移除非字母、数字和空格的字符\n",
    "    \n",
    "    # 将文本统一转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 分词并过滤停用词\n",
    "    tokens = []\n",
    "    for item in re.split(r'[:,]', text.strip()):\n",
    "        tokens.extend([t.strip() for t in item.split() if t.strip() and t.lower() not in config[\"stop_words\"]])\n",
    "    return tokens\n",
    "\n",
    "def calculate_metrics(ground_truth, extraction):\n",
    "    groundtruth_tokens = remove_stopwords_and_tokenize(ground_truth)\n",
    "    extracted_tokens = remove_stopwords_and_tokenize(extraction)\n",
    "\n",
    "    # If both are empty, return perfect metrics\n",
    "    if not groundtruth_tokens and not extracted_tokens:\n",
    "        return 1, 0, 0  # TP, FP, FN\n",
    "\n",
    "    true_positives = 0\n",
    "    used_positions = set()\n",
    "\n",
    "    for i, gt_token in enumerate(groundtruth_tokens):\n",
    "        for j, ex_token in enumerate(extracted_tokens):\n",
    "            if j not in used_positions and gt_token == ex_token:\n",
    "                true_positives += 1\n",
    "                used_positions.add(j)\n",
    "                break\n",
    "\n",
    "    false_positives = len(extracted_tokens) - true_positives\n",
    "    false_negatives = len(groundtruth_tokens) - true_positives\n",
    "\n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "def calculate_char_metrics(ground_truth, extraction):\n",
    "    # Get strings\n",
    "    groundtruth = str(ground_truth).strip()\n",
    "    extracted = str(extraction).strip()\n",
    "    \n",
    "    # If both are empty, return perfect metrics\n",
    "    if not groundtruth and not extracted:\n",
    "        return 1, 0, 0  # TP, FP, FN\n",
    "    \n",
    "    # Create character count dictionaries\n",
    "    gt_chars = {}\n",
    "    ex_chars = {}\n",
    "    \n",
    "    # Count occurrences of each character (ignoring whitespace and punctuation)\n",
    "    for char in groundtruth:\n",
    "        if char.strip() and not char in ',./:;(){}[]':\n",
    "            gt_chars[char] = gt_chars.get(char, 0) + 1\n",
    "            \n",
    "    for char in extracted:\n",
    "        if char.strip() and not char in ',./:;(){}[]':\n",
    "            ex_chars[char] = ex_chars.get(char, 0) + 1\n",
    "    \n",
    "    # Calculate true positives\n",
    "    true_positives = 0\n",
    "    for char, count in gt_chars.items():\n",
    "        if char in ex_chars:\n",
    "            true_positives += min(count, ex_chars[char])\n",
    "    \n",
    "    # Calculate false positives and false negatives\n",
    "    false_positives = sum(ex_chars.values()) - true_positives\n",
    "    false_negatives = sum(gt_chars.values()) - true_positives\n",
    "    \n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "def main():\n",
    "    # 读取 Excel 文件\n",
    "    df = pd.read_excel(config[\"xlsx_path\"])\n",
    "\n",
    "    # 确保 DOI 列、GroundTruth 列和模型列存在\n",
    "    required_columns = [config[\"doi_column\"], config[\"groundtruth_column\"], config[\"model_column\"]]\n",
    "    if not all(column in df.columns for column in required_columns):\n",
    "        print(f\"Error: Required columns {required_columns} not found in the Excel file.\")\n",
    "        return\n",
    "\n",
    "    # 对 GroundTruth 列和模型列进行预处理\n",
    "    df[config[\"groundtruth_column\"]] = df[config[\"groundtruth_column\"]].apply(\n",
    "        lambda x: process_content(x, config[\"preprocess_patterns\"])\n",
    "    )\n",
    "    df[config[\"model_column\"]] = df[config[\"model_column\"]].apply(\n",
    "        lambda x: process_content(x, config[\"preprocess_patterns\"])\n",
    "    )\n",
    "\n",
    "    # 计算每一行的 Token Level 和 Character Level 的 TP、FP、FN\n",
    "    token_tp_list, token_fp_list, token_fn_list = [], [], []\n",
    "    char_tp_list, char_fp_list, char_fn_list = [], [], []\n",
    "\n",
    "    # 去除停用词并转换为列表\n",
    "    groundtruth_tokens_list = []\n",
    "    model_tokens_list = []\n",
    "\n",
    "    # 新增：存储每行的精确度指标\n",
    "    row_metrics = []\n",
    "\n",
    "    print(\"\\n每行的评估指标:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        ground_truth = row[config[\"groundtruth_column\"]]\n",
    "        extraction = row[config[\"model_column\"]]\n",
    "        doi = row[config[\"doi_column\"]]\n",
    "\n",
    "        # 去除停用词并转换为列表\n",
    "        groundtruth_tokens = remove_stopwords_and_tokenize(ground_truth)\n",
    "        model_tokens = remove_stopwords_and_tokenize(extraction)\n",
    "        groundtruth_tokens_list.append(groundtruth_tokens)\n",
    "        model_tokens_list.append(model_tokens)\n",
    "\n",
    "        # 计算 Token Level 的 TP、FP、FN\n",
    "        token_tp, token_fp, token_fn = calculate_metrics(ground_truth, extraction)\n",
    "        token_tp_list.append(token_tp)\n",
    "        token_fp_list.append(token_fp)\n",
    "        token_fn_list.append(token_fn)\n",
    "\n",
    "        # 计算 Character Level 的 TP、FP、FN\n",
    "        char_tp, char_fp, char_fn = calculate_char_metrics(ground_truth, extraction)\n",
    "        char_tp_list.append(char_tp)\n",
    "        char_fp_list.append(char_fp)\n",
    "        char_fn_list.append(char_fn)\n",
    "\n",
    "        # 新增：计算每行的精确度指标\n",
    "        # 计算 Token-level 指标\n",
    "        token_precision = token_tp / (token_tp + token_fp) if (token_tp + token_fp) > 0 else 0\n",
    "        token_recall = token_tp / (token_tp + token_fn) if (token_tp + token_fn) > 0 else 0\n",
    "        token_f1 = (2 * token_precision * token_recall) / (token_precision + token_recall) if (token_precision + token_recall) > 0 else 0\n",
    "        token_accuracy = token_tp / (token_tp + token_fp + token_fn) if (token_tp + token_fp + token_fn) > 0 else 0\n",
    "\n",
    "        # 计算 Character-level 指标\n",
    "        char_precision = char_tp / (char_tp + char_fp) if (char_tp + char_fp) > 0 else 0\n",
    "        char_recall = char_tp / (char_tp + char_fn) if (char_tp + char_fn) > 0 else 0\n",
    "        char_f1 = (2 * char_precision * char_recall) / (char_precision + char_recall) if (char_precision + char_recall) > 0 else 0\n",
    "        char_accuracy = char_tp / (char_tp + char_fp + char_fn) if (char_tp + char_fp + char_fn) > 0 else 0\n",
    "\n",
    "        row_metrics.append({\n",
    "            \"DOI\": doi,\n",
    "            \"Token_Precision\": token_precision,\n",
    "            \"Token_Recall\": token_recall,\n",
    "            \"Token_F1\": token_f1,\n",
    "            \"Token_Accuracy\": token_accuracy,\n",
    "            \"Char_Precision\": char_precision,\n",
    "            \"Char_Recall\": char_recall,\n",
    "            \"Char_F1\": char_f1,\n",
    "            \"Char_Accuracy\": char_accuracy\n",
    "        })\n",
    "        \n",
    "        # 新增：打印每行的指标\n",
    "        # print(f\"DOI: {doi}\")\n",
    "        # print(f\"GroundTruth: {ground_truth}\")\n",
    "        # print(f\"Extraction: {extraction}\")\n",
    "        # print(\"Token-level 指标:\")\n",
    "        # print(f\"  Precision: {token_precision:.4f}, Recall: {token_recall:.4f}, F1: {token_f1:.4f}, Accuracy: {token_accuracy:.4f}\")\n",
    "        # print(f\"  TP: {token_tp}, FP: {token_fp}, FN: {token_fn}\")\n",
    "        # print(\"Character-level 指标:\")\n",
    "        # print(f\"  Precision: {char_precision:.4f}, Recall: {char_recall:.4f}, F1: {char_f1:.4f}, Accuracy: {char_accuracy:.4f}\")\n",
    "        # print(f\"  TP: {char_tp}, FP: {char_fp}, FN: {char_fn}\")\n",
    "        # print(\"-\" * 80)\n",
    "\n",
    "    # 创建一个新的 DataFrame，包含 DOI 列、处理后的 GroundTruth 列和模型列\n",
    "    result_df = pd.DataFrame({\n",
    "        config[\"doi_column\"]: df[config[\"doi_column\"]],\n",
    "        config[\"groundtruth_column\"]: groundtruth_tokens_list,\n",
    "        config[\"model_column\"]: model_tokens_list,\n",
    "        \"Token_TP\": token_tp_list,\n",
    "        \"Token_FP\": token_fp_list,\n",
    "        \"Token_FN\": token_fn_list,\n",
    "        \"Char_TP\": char_tp_list,\n",
    "        \"Char_FP\": char_fp_list,\n",
    "        \"Char_FN\": char_fn_list\n",
    "    })\n",
    "\n",
    "    # 计算整体指标\n",
    "    total_token_tp = sum(token_tp_list)\n",
    "    total_token_fp = sum(token_fp_list)\n",
    "    total_token_fn = sum(token_fn_list)\n",
    "    total_char_tp = sum(char_tp_list)\n",
    "    total_char_fp = sum(char_fp_list)\n",
    "    total_char_fn = sum(char_fn_list)\n",
    "\n",
    "    # 新增：计算整体精确度指标\n",
    "    # 计算 Token-level 整体指标\n",
    "    total_token_precision = total_token_tp / (total_token_tp + total_token_fp) if (total_token_tp + total_token_fp) > 0 else 0\n",
    "    total_token_recall = total_token_tp / (total_token_tp + total_token_fn) if (total_token_tp + total_token_fn) > 0 else 0\n",
    "    total_token_f1 = (2 * total_token_precision * total_token_recall) / (total_token_precision + total_token_recall) if (total_token_precision + total_token_recall) > 0 else 0\n",
    "    total_token_accuracy = total_token_tp / (total_token_tp + total_token_fp + total_token_fn) if (total_token_tp + total_token_fp + total_token_fn) > 0 else 0\n",
    "\n",
    "    # 计算 Character-level 整体指标\n",
    "    total_char_precision = total_char_tp / (total_char_tp + total_char_fp) if (total_char_tp + total_char_fp) > 0 else 0\n",
    "    total_char_recall = total_char_tp / (total_char_tp + total_char_fn) if (total_char_tp + total_char_fn) > 0 else 0\n",
    "    total_char_f1 = (2 * total_char_precision * total_char_recall) / (total_char_precision + total_char_recall) if (total_char_precision + total_char_recall) > 0 else 0\n",
    "    total_char_accuracy = total_char_tp / (total_char_tp + total_char_fp + total_char_fn) if (total_char_tp + total_char_fp + total_char_fn) > 0 else 0\n",
    "\n",
    "    # 将整体指标添加到 DataFrame 的最后几行\n",
    "    overall_metrics = pd.DataFrame({\n",
    "        config[\"doi_column\"]: [\"Overall Metrics\"],\n",
    "        config[\"groundtruth_column\"]: [\"\"],\n",
    "        config[\"model_column\"]: [\"\"],\n",
    "        \"Token_TP\": [total_token_tp],\n",
    "        \"Token_FP\": [total_token_fp],\n",
    "        \"Token_FN\": [total_token_fn],\n",
    "        \"Char_TP\": [total_char_tp],\n",
    "        \"Char_FP\": [total_char_fp],\n",
    "        \"Char_FN\": [total_char_fn]\n",
    "    })\n",
    "    result_df = pd.concat([result_df, overall_metrics], ignore_index=True)\n",
    "\n",
    "    # 保存为 Excel 文件\n",
    "    result_df.to_excel(config[\"output_xlsx_path\"], index=False)\n",
    "    print(f\"Excel 文件已保存至 {config['output_xlsx_path']}\")\n",
    "    \n",
    "    # 新增：打印整体的评估指标\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(\"Token-level Metrics:\")\n",
    "    print(f\"Token Precision: {total_token_precision:.4f}\")\n",
    "    print(f\"Token Recall: {total_token_recall:.4f}\")\n",
    "    print(f\"Token F1-Score: {total_token_f1:.4f}\")\n",
    "    print(f\"Token Accuracy: {total_token_accuracy:.4f}\")\n",
    "    print(f\"Token TP: {total_token_tp}, FP: {total_token_fp}, FN: {total_token_fn}\")\n",
    "\n",
    "    print(\"\\nCharacter-level Metrics:\")\n",
    "    print(f\"Char Precision: {total_char_precision:.4f}\")\n",
    "    print(f\"Char Recall: {total_char_recall:.4f}\")\n",
    "    print(f\"Char F1-Score: {total_char_f1:.4f}\")\n",
    "    print(f\"Char Accuracy: {total_char_accuracy:.4f}\")\n",
    "    print(f\"Char TP: {total_char_tp}, FP: {total_char_fp}, FN: {total_char_fn}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946a8937084e48249aaa3a4287d5fc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbc8933c7b541c38feb5fd6da36b319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a2ecb927234bbba82691f96eee64df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b477d836b39442199dde86d4c73d0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e91380a7654f75aa23052a14684e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2: Token count = 84\n",
      "Row 3: Token count = 84\n",
      "Row 4: Token count = 45\n",
      "Row 5: Token count = 48\n",
      "Row 6: Token count = 68\n",
      "Row 7: Token count = 49\n",
      "Row 8: Token count = 75\n",
      "Row 9: Token count = 75\n",
      "Row 10: Token count = 74\n",
      "Row 11: Token count = 75\n",
      "Row 12: Token count = 75\n",
      "Row 13: Token count = 172\n",
      "Row 14: Token count = 173\n",
      "Row 15: Token count = 173\n",
      "Row 16: Token count = 173\n",
      "Row 17: Token count = 162\n",
      "Row 18: Token count = 152\n",
      "Row 19: Token count = 153\n",
      "Row 20: Token count = 152\n",
      "Row 21: Token count = 143\n",
      "Row 22: Token count = 145\n",
      "Row 23: Token count = 146\n",
      "Row 24: Token count = 158\n",
      "Row 25: Token count = 158\n",
      "Row 26: Token count = 154\n",
      "Row 27: Token count = 210\n",
      "Row 28: Token count = 47\n",
      "Row 29: Token count = 47\n",
      "Row 30: Token count = 47\n",
      "Row 31: Token count = 53\n",
      "Row 32: Token count = 53\n",
      "Row 33: Token count = 53\n",
      "Row 34: Token count = 53\n",
      "Row 35: Token count = 53\n",
      "Row 36: Token count = 53\n",
      "Row 37: Token count = 53\n",
      "Row 38: Token count = 53\n",
      "Row 39: Token count = 86\n",
      "Row 40: Token count = 96\n",
      "Row 41: Token count = 96\n",
      "Row 42: Token count = 84\n",
      "Row 43: Token count = 85\n",
      "Row 44: Token count = 119\n",
      "Row 45: Token count = 129\n",
      "Row 46: Token count = 129\n",
      "Row 47: Token count = 125\n",
      "Row 48: Token count = 105\n",
      "Row 49: Token count = 139\n",
      "Row 50: Token count = 87\n",
      "Row 51: Token count = 84\n",
      "Row 52: Token count = 87\n",
      "Row 53: Token count = 87\n",
      "Row 54: Token count = 101\n",
      "Row 55: Token count = 85\n",
      "Row 56: Token count = 86\n",
      "Row 57: Token count = 86\n",
      "Row 58: Token count = 86\n",
      "Row 59: Token count = 86\n",
      "Row 60: Token count = 88\n",
      "Row 61: Token count = 88\n",
      "Row 62: Token count = 88\n",
      "Row 63: Token count = 88\n",
      "Row 64: Token count = 88\n",
      "Row 65: Token count = 115\n",
      "Row 66: Token count = 116\n",
      "Row 67: Token count = 125\n",
      "Row 68: Token count = 120\n",
      "Row 69: Token count = 64\n",
      "Row 70: Token count = 100\n",
      "Row 71: Token count = 123\n",
      "Row 72: Token count = 102\n",
      "Row 73: Token count = 66\n",
      "Row 74: Token count = 156\n",
      "Row 75: Token count = 162\n",
      "Row 76: Token count = 57\n",
      "Row 77: Token count = 56\n",
      "Row 78: Token count = 56\n",
      "Row 79: Token count = 59\n",
      "Row 80: Token count = 60\n",
      "Row 81: Token count = 54\n",
      "Row 82: Token count = 57\n",
      "Row 83: Token count = 55\n",
      "Row 84: Token count = 88\n",
      "Row 85: Token count = 97\n",
      "Row 86: Token count = 87\n",
      "Row 87: Token count = 89\n",
      "Row 88: Token count = 87\n",
      "Row 89: Token count = 55\n",
      "Row 90: Token count = 94\n",
      "Row 91: Token count = 96\n",
      "Row 92: Token count = 56\n",
      "Row 93: Token count = 60\n",
      "Row 94: Token count = 60\n",
      "Row 95: Token count = 61\n",
      "Row 96: Token count = 64\n",
      "Row 97: Token count = 64\n",
      "Row 98: Token count = 60\n",
      "Row 99: Token count = 65\n",
      "Row 100: Token count = 65\n",
      "Row 101: Token count = 60\n",
      "Row 102: Token count = 62\n",
      "Row 103: Token count = 59\n",
      "Row 104: Token count = 72\n",
      "Row 105: Token count = 71\n",
      "Row 106: Token count = 71\n",
      "Row 107: Token count = 72\n",
      "Row 108: Token count = 71\n",
      "Row 109: Token count = 71\n",
      "Row 110: Token count = 76\n",
      "Row 111: Token count = 75\n",
      "Row 112: Token count = 71\n",
      "Row 113: Token count = 75\n",
      "Row 114: Token count = 73\n",
      "Row 115: Token count = 76\n",
      "Row 116: Token count = 156\n",
      "Row 117: Token count = 157\n",
      "Row 118: Token count = 106\n",
      "Row 119: Token count = 103\n",
      "Row 120: Token count = 98\n",
      "Row 121: Token count = 101\n",
      "Row 122: Token count = 99\n",
      "Row 123: Token count = 99\n",
      "Row 124: Token count = 99\n",
      "Row 125: Token count = 93\n",
      "Row 126: Token count = 94\n",
      "Row 127: Token count = 95\n",
      "Row 128: Token count = 94\n",
      "Row 129: Token count = 111\n",
      "Row 130: Token count = 114\n",
      "Row 131: Token count = 97\n",
      "Row 132: Token count = 98\n",
      "Row 133: Token count = 99\n",
      "Row 134: Token count = 98\n",
      "Row 135: Token count = 98\n",
      "Row 136: Token count = 100\n",
      "Row 137: Token count = 151\n",
      "Row 138: Token count = 151\n",
      "Row 139: Token count = 157\n",
      "Row 140: Token count = 157\n",
      "Row 141: Token count = 151\n",
      "Row 142: Token count = 151\n",
      "Row 143: Token count = 68\n",
      "Row 144: Token count = 68\n",
      "Row 145: Token count = 68\n",
      "Row 146: Token count = 68\n",
      "Row 147: Token count = 68\n",
      "Row 148: Token count = 68\n",
      "Row 149: Token count = 68\n",
      "Row 150: Token count = 67\n",
      "Row 151: Token count = 126\n",
      "Row 152: Token count = 124\n",
      "Row 153: Token count = 151\n",
      "Row 154: Token count = 133\n",
      "Row 155: Token count = 177\n",
      "Row 156: Token count = 134\n",
      "Row 157: Token count = 133\n",
      "Row 158: Token count = 134\n",
      "Row 159: Token count = 57\n",
      "Row 160: Token count = 167\n",
      "Row 161: Token count = 181\n",
      "Row 162: Token count = 181\n",
      "Row 163: Token count = 165\n",
      "Row 164: Token count = 181\n",
      "Row 165: Token count = 180\n",
      "Row 166: Token count = 45\n",
      "Row 167: Token count = 43\n",
      "Row 168: Token count = 47\n",
      "Row 169: Token count = 48\n",
      "Row 170: Token count = 42\n",
      "Row 171: Token count = 42\n",
      "Row 172: Token count = 43\n",
      "Row 173: Token count = 138\n",
      "Row 174: Token count = 136\n",
      "Row 175: Token count = 141\n",
      "Row 176: Token count = 80\n",
      "Row 177: Token count = 121\n",
      "Row 178: Token count = 138\n",
      "Row 179: Token count = 148\n",
      "Row 180: Token count = 109\n",
      "Row 181: Token count = 114\n",
      "Row 182: Token count = 105\n",
      "Row 183: Token count = 94\n",
      "Row 184: Token count = 74\n",
      "Row 185: Token count = 79\n",
      "Row 186: Token count = 68\n",
      "Row 187: Token count = 84\n",
      "Row 188: Token count = 91\n",
      "Row 189: Token count = 86\n",
      "Row 190: Token count = 91\n",
      "Row 191: Token count = 106\n",
      "Row 192: Token count = 107\n",
      "Row 193: Token count = 106\n",
      "Row 194: Token count = 107\n",
      "Row 195: Token count = 107\n",
      "Row 196: Token count = 108\n",
      "Row 197: Token count = 108\n",
      "Row 198: Token count = 77\n",
      "Row 199: Token count = 205\n",
      "Row 200: Token count = 204\n",
      "Row 201: Token count = 208\n",
      "Row 202: Token count = 208\n",
      "Row 203: Token count = 149\n",
      "Row 204: Token count = 149\n",
      "Row 205: Token count = 149\n",
      "Row 206: Token count = 149\n",
      "Row 207: Token count = 149\n",
      "Row 208: Token count = 149\n",
      "Row 209: Token count = 109\n",
      "Row 210: Token count = 122\n",
      "Row 211: Token count = 126\n",
      "Row 212: Token count = 151\n",
      "Row 213: Token count = 114\n",
      "Row 214: Token count = 125\n",
      "Row 215: Token count = 90\n",
      "Row 216: Token count = 102\n",
      "Row 217: Token count = 102\n",
      "Row 218: Token count = 103\n",
      "Row 219: Token count = 109\n",
      "Row 220: Token count = 128\n",
      "Row 221: Token count = 134\n",
      "Row 222: Token count = 129\n",
      "Row 223: Token count = 129\n",
      "Row 224: Token count = 81\n",
      "Row 225: Token count = 92\n",
      "Row 226: Token count = 85\n",
      "Row 227: Token count = 89\n",
      "Row 228: Token count = 88\n",
      "Row 229: Token count = 167\n",
      "Row 230: Token count = 164\n",
      "Row 231: Token count = 165\n",
      "Row 232: Token count = 167\n",
      "Row 233: Token count = 175\n",
      "Row 234: Token count = 163\n",
      "Row 235: Token count = 168\n",
      "Row 236: Token count = 314\n",
      "Row 237: Token count = 315\n",
      "Row 238: Token count = 316\n",
      "Row 239: Token count = 312\n",
      "Row 240: Token count = 313\n",
      "Row 241: Token count = 314\n",
      "Row 242: Token count = 68\n",
      "Row 243: Token count = 68\n",
      "Row 244: Token count = 68\n",
      "Row 245: Token count = 68\n",
      "Row 246: Token count = 68\n",
      "Row 247: Token count = 145\n",
      "Row 248: Token count = 135\n",
      "Row 249: Token count = 148\n",
      "Row 250: Token count = 87\n",
      "Row 251: Token count = 90\n",
      "Row 252: Token count = 84\n",
      "Row 253: Token count = 89\n",
      "Row 254: Token count = 84\n",
      "Row 255: Token count = 84\n",
      "Row 256: Token count = 131\n",
      "Row 257: Token count = 207\n",
      "Row 258: Token count = 131\n",
      "Row 259: Token count = 131\n",
      "Row 260: Token count = 128\n",
      "Row 261: Token count = 162\n",
      "Row 262: Token count = 165\n",
      "Row 263: Token count = 164\n",
      "Row 264: Token count = 112\n",
      "Row 265: Token count = 113\n",
      "Row 266: Token count = 148\n",
      "Row 267: Token count = 154\n",
      "Row 268: Token count = 151\n",
      "Row 269: Token count = 151\n",
      "Row 270: Token count = 86\n",
      "Row 271: Token count = 149\n",
      "Row 272: Token count = 160\n",
      "Row 273: Token count = 160\n",
      "Row 274: Token count = 92\n",
      "Row 275: Token count = 145\n",
      "Row 276: Token count = 168\n",
      "Row 277: Token count = 154\n",
      "Row 278: Token count = 156\n",
      "Row 279: Token count = 45\n",
      "Row 280: Token count = 48\n",
      "Row 281: Token count = 34\n",
      "Row 282: Token count = 49\n",
      "Row 283: Token count = 182\n",
      "Row 284: Token count = 230\n",
      "Row 285: Token count = 187\n",
      "Row 286: Token count = 189\n",
      "Row 287: Token count = 187\n",
      "Row 288: Token count = 189\n",
      "Row 289: Token count = 92\n",
      "Row 290: Token count = 93\n",
      "Row 291: Token count = 111\n",
      "Row 292: Token count = 106\n",
      "Row 293: Token count = 91\n",
      "Row 294: Token count = 80\n",
      "Row 295: Token count = 76\n",
      "Row 296: Token count = 72\n",
      "Row 297: Token count = 89\n",
      "Row 298: Token count = 75\n",
      "Row 299: Token count = 66\n",
      "Row 300: Token count = 61\n",
      "Row 301: Token count = 166\n",
      "Row 302: Token count = 139\n",
      "Row 303: Token count = 140\n",
      "Row 304: Token count = 139\n",
      "Row 305: Token count = 138\n",
      "Row 306: Token count = 139\n",
      "Row 307: Token count = 139\n",
      "Row 308: Token count = 138\n",
      "Row 309: Token count = 110\n",
      "Row 310: Token count = 108\n",
      "Row 311: Token count = 117\n",
      "Row 312: Token count = 112\n",
      "Row 313: Token count = 87\n",
      "Row 314: Token count = 89\n",
      "Row 315: Token count = 89\n",
      "Row 316: Token count = 97\n",
      "Row 317: Token count = 100\n",
      "Row 318: Token count = 102\n",
      "Row 319: Token count = 95\n",
      "Row 320: Token count = 96\n",
      "Row 321: Token count = 82\n",
      "Row 322: Token count = 82\n",
      "Row 323: Token count = 82\n",
      "Row 324: Token count = 82\n",
      "Row 325: Token count = 82\n",
      "Row 326: Token count = 82\n",
      "Row 327: Token count = 82\n",
      "Row 328: Token count = 115\n",
      "Row 329: Token count = 82\n",
      "Row 330: Token count = 84\n",
      "Row 331: Token count = 138\n",
      "Row 332: Token count = 139\n",
      "Row 333: Token count = 107\n",
      "Row 334: Token count = 64\n",
      "Row 335: Token count = 93\n",
      "Row 336: Token count = 85\n",
      "Row 337: Token count = 50\n",
      "Row 338: Token count = 42\n",
      "Row 339: Token count = 111\n",
      "Row 340: Token count = 111\n",
      "Row 341: Token count = 57\n",
      "Row 342: Token count = 50\n",
      "Row 343: Token count = 89\n",
      "Row 344: Token count = 91\n",
      "Row 345: Token count = 69\n",
      "Row 346: Token count = 121\n",
      "Row 347: Token count = 132\n",
      "Row 348: Token count = 117\n",
      "Row 349: Token count = 48\n",
      "Row 350: Token count = 100\n",
      "Row 351: Token count = 98\n",
      "Row 352: Token count = 99\n",
      "Row 353: Token count = 99\n",
      "Row 354: Token count = 127\n",
      "Row 355: Token count = 127\n",
      "Row 356: Token count = 79\n",
      "Row 357: Token count = 83\n",
      "Row 358: Token count = 62\n",
      "Row 359: Token count = 48\n",
      "Row 360: Token count = 48\n",
      "Row 361: Token count = 61\n",
      "Row 362: Token count = 58\n",
      "Row 363: Token count = 76\n",
      "Row 364: Token count = 74\n",
      "Row 365: Token count = 70\n",
      "Row 366: Token count = 85\n",
      "Row 367: Token count = 84\n",
      "Row 368: Token count = 72\n",
      "Row 369: Token count = 72\n",
      "Row 370: Token count = 73\n",
      "Row 371: Token count = 49\n",
      "Row 372: Token count = 70\n",
      "Row 373: Token count = 83\n",
      "Row 374: Token count = 80\n",
      "Row 375: Token count = 182\n",
      "Row 376: Token count = 182\n",
      "Row 377: Token count = 174\n",
      "Row 378: Token count = 125\n",
      "Row 379: Token count = 117\n",
      "Row 380: Token count = 107\n",
      "Row 381: Token count = 100\n",
      "Row 382: Token count = 97\n",
      "Row 383: Token count = 98\n",
      "Row 384: Token count = 55\n",
      "Row 385: Token count = 103\n",
      "Row 386: Token count = 104\n",
      "Row 387: Token count = 64\n",
      "Row 388: Token count = 70\n",
      "Row 389: Token count = 71\n",
      "Row 390: Token count = 70\n",
      "Row 391: Token count = 70\n",
      "Row 392: Token count = 70\n",
      "Row 393: Token count = 70\n",
      "Row 394: Token count = 71\n",
      "Row 395: Token count = 70\n",
      "Row 396: Token count = 66\n",
      "Row 397: Token count = 117\n",
      "Row 398: Token count = 107\n",
      "Row 399: Token count = 109\n",
      "Row 400: Token count = 100\n",
      "Row 401: Token count = 101\n",
      "Row 402: Token count = 98\n",
      "Row 403: Token count = 91\n",
      "Row 404: Token count = 164\n",
      "Row 405: Token count = 173\n",
      "Row 406: Token count = 173\n",
      "Row 407: Token count = 94\n",
      "Row 408: Token count = 94\n",
      "Row 409: Token count = 94\n",
      "Row 410: Token count = 94\n",
      "Row 411: Token count = 91\n",
      "Row 412: Token count = 94\n",
      "Row 413: Token count = 141\n",
      "Row 414: Token count = 141\n",
      "Row 415: Token count = 134\n",
      "Row 416: Token count = 134\n",
      "Row 417: Token count = 58\n",
      "Row 418: Token count = 60\n",
      "Row 419: Token count = 54\n",
      "Row 420: Token count = 76\n",
      "Row 421: Token count = 75\n",
      "Row 422: Token count = 75\n",
      "Row 423: Token count = 71\n",
      "Row 424: Token count = 71\n",
      "Row 425: Token count = 71\n",
      "Row 426: Token count = 71\n",
      "Row 427: Token count = 71\n",
      "Row 428: Token count = 71\n",
      "Row 429: Token count = 48\n",
      "Row 430: Token count = 50\n",
      "Row 431: Token count = 51\n",
      "Row 432: Token count = 49\n",
      "Row 433: Token count = 48\n",
      "Row 434: Token count = 48\n",
      "Row 435: Token count = 47\n",
      "Row 436: Token count = 51\n",
      "Row 437: Token count = 52\n",
      "Row 438: Token count = 136\n",
      "Row 439: Token count = 136\n",
      "Row 440: Token count = 137\n",
      "Row 441: Token count = 223\n",
      "Row 442: Token count = 43\n",
      "Row 443: Token count = 43\n",
      "Row 444: Token count = 44\n",
      "Row 445: Token count = 73\n",
      "Row 446: Token count = 73\n",
      "Row 447: Token count = 57\n",
      "Row 448: Token count = 49\n",
      "Row 449: Token count = 171\n",
      "Row 450: Token count = 167\n",
      "Row 451: Token count = 174\n",
      "Row 452: Token count = 173\n",
      "\n",
      "Total token count: 47451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 读取表格\n",
    "df = pd.read_csv(\"/Users/zixuanzhao/Desktop/数据治理/Metrics/Structured-Info/Results/Structural-Text/llama-3.3-70b-instruct.csv\")  # 或 pd.read_excel(\"your_file.xlsx\")\n",
    "\n",
    "# 初始化 tokenizer（以 GPT-4 为例）\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# 选择列名（比如 \"Prompt\"）\n",
    "column_name = \"Composition\"\n",
    "# column_name = \"Heat_Treatment\"\n",
    "# column_name = \"Deformation/Manufacturing_Technique\"\n",
    "# column_name = \"Microstructure1\"\n",
    "# column_name = \"Microstructure2\"\n",
    "# column_name = \"Properties\"\n",
    "# column_name = \"Performance\"\n",
    "# column_name = \"Strengthening_Mechanism\"\n",
    "\n",
    "# ✅ 初始化总 token 计数器\n",
    "total_tokens = 0\n",
    "\n",
    "# 从第二行开始逐行计算\n",
    "for i, text in enumerate(df[column_name].iloc[1:], start=2):\n",
    "    if pd.notna(text):\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        total_tokens += num_tokens\n",
    "        print(f\"Row {i}: Token count = {num_tokens}\")\n",
    "        \n",
    "# ✅ 打印总 token 数\n",
    "print(f\"\\nTotal token count: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 合并完成，共合并 243 个文件，输出路径：/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Abstract2/merged_Table.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path1 = '/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Table/untitled folder'  # 输入目录\n",
    "path2 = '/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Abstract2'        # 输出目录\n",
    "output_filename = \"merged_Table.json\"\n",
    "\n",
    "def read_file_with_encoding_fallback(file_path):\n",
    "    # 尝试 utf-8 和 latin-1 两种编码\n",
    "    for enc in ['utf-8', 'latin-1']:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"❌ 无法读取文件: {file_path}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    all_data = []\n",
    "\n",
    "    for file in os.listdir(path1):\n",
    "        if file.endswith(\".json\") and not file.startswith(\".\"):\n",
    "            file_path = os.path.join(path1, file)\n",
    "            content = read_file_with_encoding_fallback(file_path)\n",
    "            if content is not None:\n",
    "                all_data.append({\"File\": content})\n",
    "\n",
    "    # 创建输出目录\n",
    "    os.makedirs(path2, exist_ok=True)\n",
    "    output_path = os.path.join(path2, output_filename)\n",
    "\n",
    "    # 写入合并结果\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(all_data, f_out, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ 合并完成，共合并 {len(all_data)} 个文件，输出路径：{output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 文件 /Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Combined/merged_OCR+LLM.json 的总 token 数为: 1517\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import tiktoken\n",
    "\n",
    "# # 设定 JSON 文件路径\n",
    "file_path = \"/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Combined/merged_OCR+LLM.json\"\n",
    "\n",
    "# # 初始化 tokenizer（以 GPT-4o 为例）\n",
    "# encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# encoding = tiktoken.encoding_for_model(\"gpt-4-1106-preview\")\n",
    "# 读取 JSON 文件内容为纯文本\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()  # 不做 json.load，而是直接读取整个文本内容\n",
    "\n",
    "# 计算 token 数\n",
    "token_count = len(encoding.encode(content))\n",
    "\n",
    "# 输出 token 数\n",
    "print(f\"✅ 文件 {file_path} 的总 token 数为: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at '/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Combined/merged_OCR+LLM.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: 加载 Word 文档\u001b[39;00m\n\u001b[1;32m      5\u001b[0m doc_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Combined/merged_OCR+LLM.json\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 修改为你实际文件路径\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mdocx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: 提取并合并所有段落文本\u001b[39;00m\n\u001b[1;32m      9\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([para\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs \u001b[38;5;28;01mif\u001b[39;00m para\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/docx/api.py:27\u001b[0m, in \u001b[0;36mDocument\u001b[0;34m(docx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mloaded.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m docx \u001b[38;5;241m=\u001b[39m _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[0;32m---> 27\u001b[0m document_part \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentPart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mPackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain_document_part)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[1;32m     29\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/docx/opc/package.py:127\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OpcPackage:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     pkg_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPackageReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    129\u001b[0m     Unmarshaller\u001b[38;5;241m.\u001b[39munmarshal(pkg_reader, package, PartFactory)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/docx/opc/pkgreader.py:22\u001b[0m, in \u001b[0;36mPackageReader.from_file\u001b[0;34m(pkg_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(pkg_file):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     phys_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPhysPkgReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(phys_reader\u001b[38;5;241m.\u001b[39mcontent_types_xml)\n\u001b[1;32m     24\u001b[0m     pkg_srels \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39m_srels_for(phys_reader, PACKAGE_URI)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/docx/opc/phys_pkg.py:21\u001b[0m, in \u001b[0;36mPhysPkgReader.__new__\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m         reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage not found at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pkg_file)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume it's a stream and pass it to Zip reader to sort out\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: Package not found at '/Users/zixuanzhao/Desktop/数据治理/Metrics/Unstructured-Info/Combined/merged_OCR+LLM.json'"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import tiktoken\n",
    "\n",
    "# Step 1: 加载 Word 文档\n",
    "doc_path = \"...\"  # 修改为你实际文件路径\n",
    "doc = docx.Document(doc_path)\n",
    "\n",
    "# Step 2: 提取并合并所有段落文本\n",
    "full_text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Step 3: 初始化 tokenizer（GPT-4o）\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4-1106-preview\")\n",
    "\n",
    "# Step 4: 计算 token 数\n",
    "token_count = len(encoding.encode(full_text))\n",
    "\n",
    "# 输出\n",
    "print(f\"✅ 总 token 数: {token_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
