{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Simplify Table from XML/HTML formats to Customized Table format and Save as JSON Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify Table from Elsevier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "def parse(path):\n",
    "    file_paths = traverse_folder(path)\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.xml'):\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            root_str = ET.tostring(root).decode().replace('\\n', '')\n",
    "            soup = BeautifulSoup(root_str, 'xml')\n",
    "            tables = soup.findAll('table')\n",
    "            count = 1\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    info = {}\n",
    "                    title = ''\n",
    "                    label = table.find('label')\n",
    "                    if label and label.get_text():\n",
    "                        title = title + label.get_text() + \": \"\n",
    "                    simple_para = table.find(\"simple-para\")\n",
    "                    if simple_para and  len(simple_para.get_text()) > 0:\n",
    "                        title = title + simple_para.get_text() + \" \"\n",
    "                    info['title'] = title.replace(' ', '')\n",
    "                    thead = table.find('thead')\n",
    "                    rows_list = parse_row(thead)\n",
    "                    if len(rows_list) > 0:\n",
    "                        rows_list[0][0] = '<thead>' + rows_list[0][0]\n",
    "                        rows_last = rows_list[len(rows_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</thead>'\n",
    "\n",
    "                    tbody = table.find('tbody')\n",
    "                    tbody_row_list = parse_row(tbody)\n",
    "                    if len(tbody_row_list) > 0:\n",
    "                        tbody_row_list[0][0] = '<tbody>' + tbody_row_list[0][0]\n",
    "                        rows_last = tbody_row_list[len(tbody_row_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</tbody>'\n",
    "                    rows_list.append(tbody_row_list)\n",
    "                    info['values'] = rows_list\n",
    "                    table_name = f'table{count}.json'\n",
    "                    save(file_path, table_name, info)\n",
    "                    count += 1\n",
    "                except Exception:\n",
    "                    copy_fail_file(file_path)\n",
    "                    break\n",
    "\n",
    "def parse_row(tag):\n",
    "    rows_list = []\n",
    "    if tag:\n",
    "        rows = tag.findAll('row')\n",
    "        for row in rows:\n",
    "            entry_list = []\n",
    "            entries = row.findAll('entry')\n",
    "            for entry in entries:\n",
    "                entry_str = '<entry'\n",
    "                text = entry.get_text()\n",
    "                attrs = entry.attrs\n",
    "                if attrs:\n",
    "                    if 'morerows' in attrs:\n",
    "                        entry_str = entry_str + ' morerows=' + attrs['morerows'].replace('\"', '')\n",
    "                    if 'namest' in attrs:\n",
    "                        entry_str = entry_str + ' namest=' + attrs['namest'].replace('\"', '')\n",
    "                    if 'nameend' in attrs:\n",
    "                        entry_str = entry_str + ' nameend=' + attrs['nameend'].replace('\"', '')\n",
    "                    if 'role' in attrs:\n",
    "                        entry_str = entry_str + ' role=' + attrs['role'].replace('\"', '')\n",
    "                if text:\n",
    "                    entry_str = entry_str + \":\" + text.strip().replace(' ', '')\n",
    "                entry_str = entry_str + \">\"\n",
    "                entry_list.append(entry_str)\n",
    "            if len(entry_list) > 0:\n",
    "                entry_list[0] = '<row>' + entry_list[0]\n",
    "                entry_list[len(entry_list) - 1] = entry_list[len(entry_list) - 1] + '</row>'\n",
    "                rows_list.append(entry_list)\n",
    "    return rows_list\n",
    "\n",
    "def traverse_folder(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            abs_file_path = os.path.join(root, file_name)\n",
    "            file_paths.append(abs_file_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "def copy_fail_file(file_path: str):\n",
    "    path = generate_path(file_path, 'Failed-Table') ### Fail2Indentified Tables saved path\n",
    "    rfind = path.rfind(os.sep)\n",
    "    path = path[:rfind]\n",
    "    create_file(path)\n",
    "    shutil.copy(file_path, path)\n",
    "\n",
    "def save(file_path: str, table_name: str, info):\n",
    "    if info:\n",
    "        path = generate_path(file_path, 'E-Table') ### Table saved path\n",
    "        create_file(path)\n",
    "        file_path_ = os.path.join(path, table_name)\n",
    "\n",
    "        with open(file_path_, 'w', encoding='utf-8') as fp:\n",
    "            json.dump(info, fp, ensure_ascii=False)\n",
    "\n",
    "def generate_path(file_path, folder_name):\n",
    "    index = file_path.rfind(os.sep)\n",
    "    path_ = file_path[:index]\n",
    "    index = path_.rfind(os.sep)\n",
    "    path_ = path_[:index]\n",
    "    file_name_ = file_path[index + 1:]\n",
    "    last_dot = file_name_.rfind(\".\")\n",
    "    file_name = file_name_[:last_dot]\n",
    "    rfind = file_name.rfind(os.sep)\n",
    "    file_name = file_name[rfind + 1:]\n",
    "    path = os.path.join(path_.strip(), folder_name, file_name.strip())\n",
    "    return path\n",
    "\n",
    "def create_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = os.path.join('Data', 'E')  ### File Path and Publisher Folder\n",
    "    parse(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify Table from Royal Soc Chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def parse(path):\n",
    "    file_paths = traverse_folder(path)\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.html'):\n",
    "            fp = open(file_path, 'r', encoding='utf-8')\n",
    "            soup = BeautifulSoup(fp, 'html.parser')\n",
    "            table_titles = soup.find_all('div', {'class': 'table_caption'})\n",
    "            tables = soup.find_all('table', {'class': ['tgroup', 'rtable']})\n",
    "            count = 0\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    info = {}\n",
    "                    title_info = table_titles[count]\n",
    "                    title_key = title_info.find('b').get_text()\n",
    "                    title = title_info.find('span').get_text()\n",
    "                    info[title_key] = title\n",
    "                    thead = table.find('thead')\n",
    "                    rows_list = parse_table(thead, 'th')\n",
    "                    if len(rows_list) > 0:\n",
    "                        rows_list[0][0] = '<thead>' + rows_list[0][0]\n",
    "                        rows_last = rows_list[len(rows_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</thead>'\n",
    "\n",
    "                    thead = table.find('tfoot')\n",
    "                    tfoot_rows_list = parse_table(thead, 'th')\n",
    "                    if len(tfoot_rows_list) > 0:\n",
    "                        tfoot_rows_list[0][0] = '<tfoot>' + tfoot_rows_list[0][0]\n",
    "                        rows_last = tfoot_rows_list[len(tfoot_rows_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</tfoot>'\n",
    "                        rows_list.append(tfoot_rows_list)\n",
    "                    tbody = table.find('tbody')\n",
    "                    tbody_row_list = parse_table(tbody, 'td')\n",
    "                    if len(tbody_row_list) > 0:\n",
    "                        tbody_row_list[0][0] = '<tbody>' + tbody_row_list[0][0]\n",
    "                        rows_last = tbody_row_list[len(tbody_row_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</tbody>'\n",
    "                        rows_list.append(tbody_row_list)\n",
    "                    info['values'] = rows_list\n",
    "                    table_name = f'table{count + 1}.json'\n",
    "                    save(file_path, table_name, info)\n",
    "                    count += 1\n",
    "                except Exception:\n",
    "                    copy_fail_file(file_path)\n",
    "                    break\n",
    "\n",
    "\n",
    "def parse_table(tag, chilld):\n",
    "    tr_list = []\n",
    "    if tag:\n",
    "        trs = tag.findAll('tr')\n",
    "        for row in trs:\n",
    "            th_list = []\n",
    "            ths = row.findAll(chilld)\n",
    "            for th in ths:\n",
    "                th_context = f'<{chilld}'\n",
    "                text = th.get_text()\n",
    "                attrs = th.attrs\n",
    "                if attrs:\n",
    "                    if 'rowspan' in attrs:\n",
    "                        th_context = th_context + ' rowspan=' + attrs['rowspan'].replace('\"', '')\n",
    "                    if 'colspan' in attrs:\n",
    "                        th_context = th_context + ' colspan=' + attrs['colspan'].replace('\"', '')\n",
    "                if text:\n",
    "                    th_context = th_context + \":\" + text\n",
    "                th_context = th_context + f\">\"\n",
    "                th_list.append(th_context)\n",
    "            if len(th_list) > 0:\n",
    "                th_list[0] = '<tr>' + th_list[0]\n",
    "                th_list[len(th_list) - 1] = th_list[len(th_list) - 1] + '</tr>'\n",
    "                tr_list.append(th_list)\n",
    "    return tr_list\n",
    "\n",
    "\n",
    "def traverse_folder(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            abs_file_path = os.path.join(root, file_name)\n",
    "            file_paths.append(abs_file_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def copy_fail_file(file_path:str):\n",
    "    path = generate_path(file_path, 'Failed-Table') ### Fail2Indentified Tables saved path\n",
    "    rfind = path.rfind(os.sep)\n",
    "    path = path[:rfind]\n",
    "    create_file(path)\n",
    "    shutil.copy(file_path, path)\n",
    "\n",
    "\n",
    "def save(file_path: str, table_name: str, info):\n",
    "    if info:\n",
    "        path = generate_path(file_path,'RSC-Table') ### Table saved path\n",
    "        create_file(path)\n",
    "        file_path_ = os.path.join(path, table_name)\n",
    "\n",
    "        with open(file_path_, 'w', encoding='utf-8') as fp:\n",
    "            json.dump(info, fp, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def generate_path(file_path,folder_name):\n",
    "    index = file_path.rfind(os.sep)\n",
    "    path_ = file_path[:index]\n",
    "    index = path_.rfind(os.sep)\n",
    "    path_ = path_[:index]\n",
    "    file_name_ = file_path[index + 1:]\n",
    "    last_dot = file_name_.rfind(\".\")\n",
    "    file_name = file_name_[:last_dot]\n",
    "    rfind = file_name.rfind(os.sep)\n",
    "    file_name = file_name[rfind + 1:]\n",
    "    path = os.path.join(path_.strip(), folder_name, file_name.strip())\n",
    "    return path\n",
    "\n",
    "\n",
    "def create_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = os.path.join('Data', 'RSC') ### File Path and Publisher Folder\n",
    "    parse(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify Table from Springer Nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def parse(path):\n",
    "    file_paths = traverse_folder(path)\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.xml'):\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            root_str = ET.tostring(root).decode().replace('\\n', '')\n",
    "            soup = BeautifulSoup(root_str, 'xml')\n",
    "            tables = soup.findAll('table-wrap')\n",
    "            count = 1\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    info = {}\n",
    "                    label = table.find('label').get_text()\n",
    "                    title = table.find(\"p\").get_text().replace(' ', '')\n",
    "                    info[label] = title\n",
    "                    thead = table.find('thead')\n",
    "                    rows_list = parse_table(thead, 'th')\n",
    "                    if len(rows_list) > 0:\n",
    "                        rows_list[0][0] = '<thead>' + rows_list[0][0]\n",
    "                        rows_last = rows_list[len(rows_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</thead>'\n",
    "\n",
    "                    tbody = table.find('tbody')\n",
    "                    tbody_row_list = parse_table(tbody, 'td')\n",
    "                    if len(tbody_row_list) > 0:\n",
    "                        tbody_row_list[0][0] = '<tbody>' + tbody_row_list[0][0]\n",
    "                        rows_last = tbody_row_list[len(tbody_row_list) - 1]\n",
    "                        rows_last[len(rows_last) - 1] = rows_last[len(rows_last) - 1] + '</tbody>'\n",
    "                    rows_list.append(tbody_row_list)\n",
    "                    info['values'] = rows_list\n",
    "                    table_name = f'table{count}.json'\n",
    "                    save(file_path, table_name,info)\n",
    "                    count += 1\n",
    "                except Exception:\n",
    "                    copy_fail_file(file_path)\n",
    "                    break\n",
    "\n",
    "\n",
    "def parse_table(tag, chilld):\n",
    "    tr_list = []\n",
    "    if tag:\n",
    "        trs = tag.findAll('tr')\n",
    "        for row in trs:\n",
    "            th_list = []\n",
    "            ths = row.findAll(chilld)\n",
    "            for th in ths:\n",
    "                th_context = f'<{chilld}'\n",
    "                text = th.get_text()\n",
    "                attrs = th.attrs\n",
    "                if attrs:\n",
    "                    if 'rowspan' in attrs:\n",
    "                        th_context = th_context + ' rowspan=' + attrs['rowspan'].replace('\"', '')\n",
    "                    if 'colspan' in attrs:\n",
    "                        th_context = th_context + ' colspan=' + attrs['colspan'].replace('\"', '')\n",
    "                if text:\n",
    "                    th_context = th_context + \":\" + text.strip()\n",
    "                th_context = th_context + f\">\"\n",
    "                th_list.append(th_context)\n",
    "            if len(th_list) > 0:\n",
    "                th_list[0] = '<tr>' + th_list[0]\n",
    "                th_list[len(th_list) - 1] = th_list[len(th_list) - 1] + '</tr>'\n",
    "                tr_list.append(th_list)\n",
    "    return tr_list\n",
    "\n",
    "\n",
    "def traverse_folder(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            abs_file_path = os.path.join(root, file_name)\n",
    "            file_paths.append(abs_file_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def copy_fail_file(file_path:str):\n",
    "    path = generate_path(file_path, 'Failed-Table') ### Fail2Indentified Tables saved path\n",
    "    rfind = path.rfind(os.sep)\n",
    "    path = path[:rfind]\n",
    "    create_file(path)\n",
    "    shutil.copy(file_path, path)\n",
    "\n",
    "\n",
    "def save(file_path: str, table_name: str, info):\n",
    "    if info:\n",
    "        path = generate_path(file_path,'SN-Table') ### Table saved path\n",
    "        create_file(path)\n",
    "        file_path_ = os.path.join(path, table_name)\n",
    "\n",
    "        with open(file_path_, 'w', encoding='utf-8') as fp:\n",
    "            json.dump(info, fp, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def generate_path(file_path,folder_name):\n",
    "    index = file_path.rfind(os.sep)\n",
    "    path_ = file_path[:index]\n",
    "    index = path_.rfind(os.sep)\n",
    "    path_ = path_[:index]\n",
    "    file_name_ = file_path[index + 1:]\n",
    "    last_dot = file_name_.rfind(\".\")\n",
    "    file_name = file_name_[:last_dot]\n",
    "    rfind = file_name.rfind(os.sep)\n",
    "    file_name = file_name[rfind + 1:]\n",
    "    path = os.path.join(path_.strip(), folder_name, file_name.strip())\n",
    "    return path\n",
    "\n",
    "\n",
    "def create_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = os.path.join('Data', 'SN') ### File Path and Publisher Folder\n",
    "    parse(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special XML Table Format Without Thead in SpringerNature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def parse(path):\n",
    "    file_paths = traverse_folder(path)\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.xml'):\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            root_str = ET.tostring(root).decode().replace('\\n', '')\n",
    "            soup = BeautifulSoup(root_str, 'xml')\n",
    "            tables = soup.findAll('table-wrap')\n",
    "            count = 1\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    info = {}\n",
    "                    label = table.find('label').get_text()\n",
    "                    title = table.find(\"p\").get_text().replace(' ', '')\n",
    "                    info[label] = title\n",
    "\n",
    "                    # 如果 <thead> 存在，保留它，否则跳过处理\n",
    "                    thead_tag = table.find('thead')\n",
    "                    if thead_tag:\n",
    "                        thead = str(thead_tag)\n",
    "                        info['thead'] = thead\n",
    "                    else:\n",
    "                        info['thead'] = None\n",
    "\n",
    "                    # 处理 <tbody> 内容\n",
    "                    tbody = table.find('tbody')\n",
    "                    tbody_row_list = parse_table(tbody, 'td')\n",
    "                    if tbody_row_list:\n",
    "                        tbody_row_list[0][0] = '<tbody>' + tbody_row_list[0][0]\n",
    "                        tbody_row_list[-1][-1] = tbody_row_list[-1][-1] + '</tbody>'\n",
    "\n",
    "                    info['tbody'] = tbody_row_list\n",
    "                    table_name = f'table{count}.json'\n",
    "                    save(file_path, table_name, info)\n",
    "                    count += 1\n",
    "                except Exception:\n",
    "                    copy_fail_file(file_path)\n",
    "                    break\n",
    "\n",
    "def parse_table(tag, child):\n",
    "    tr_list = []\n",
    "    if tag:\n",
    "        trs = tag.findAll('tr')\n",
    "        for row in trs:\n",
    "            td_list = []\n",
    "            tds = row.findAll(child)\n",
    "            for td in tds:\n",
    "                td_context = f'<{child}'\n",
    "                text = td.get_text()\n",
    "                attrs = td.attrs\n",
    "                if attrs:\n",
    "                    if 'rowspan' in attrs:\n",
    "                        td_context += f' rowspan={attrs[\"rowspan\"]}'\n",
    "                    if 'colspan' in attrs:\n",
    "                        td_context += f' colspan={attrs[\"colspan\"]}'\n",
    "                if text:\n",
    "                    td_context += f':{text.strip()}'\n",
    "                td_context += f'>'\n",
    "                td_list.append(td_context)\n",
    "            if td_list:\n",
    "                td_list[0] = '<tr>' + td_list[0]\n",
    "                td_list[-1] = td_list[-1] + '</tr>'\n",
    "                tr_list.append(td_list)\n",
    "    return tr_list\n",
    "\n",
    "def traverse_folder(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            abs_file_path = os.path.join(root, file_name)\n",
    "            file_paths.append(abs_file_path)\n",
    "    return file_paths\n",
    "\n",
    "def copy_fail_file(file_path: str):\n",
    "    path = generate_path(file_path, 'Failed-Table')  ### Fail2Indentified Tables saved path\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    shutil.copy(file_path, path)\n",
    "\n",
    "def save(file_path: str, table_name: str, info):\n",
    "    if info:\n",
    "        path = generate_path(file_path, 'SN-Table') ### Table saved path\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        file_path_ = os.path.join(path, table_name)\n",
    "\n",
    "        with open(file_path_, 'w', encoding='utf-8') as fp:\n",
    "            json.dump(info, fp, ensure_ascii=False)\n",
    "\n",
    "def generate_path(file_path, folder_name):\n",
    "    path_ = os.path.dirname(os.path.dirname(file_path))\n",
    "    file_name_ = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    return os.path.join(path_, folder_name, file_name_)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = 'Data/SN' ### File Path \n",
    "    parse(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
